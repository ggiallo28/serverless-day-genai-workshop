{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Creating a `.env` File and Using Environment Variables\n",
    "\n",
    "This notebook demonstrates how to create a `.env` file, install required Python packages, and load environment variables using `python-dotenv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4644ee-cf59-4f89-8083-cc89dac58210",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file .env\n",
    "AWS_ACCESS_KEY_ID=\"\"\n",
    "AWS_SECRET_ACCESS_KEY=\"\"\n",
    "AWS_SESSION_TOKEN=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-requirements",
   "metadata": {},
   "source": [
    "# Amazon Bedrock boto3 Prerequisites\n",
    "\n",
    "> This notebook is designed to run seamlessly in **`SageMaker Studio Lab`**.\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the [`boto3` Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to interact with [Amazon Bedrock](https://aws.amazon.com/bedrock/) Foundation Models. SageMaker Studio Lab provides an ideal environment for experimenting with AWS SDKs like `boto3` for machine learning and AI workflows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f473a-2929-4194-b76c-c86251f74b04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ <b>Before you get started with this workshop...</b> make sure you have manually enabled access to the following models in the \"Model access\" tab:\n",
    "\n",
    "<ul>\n",
    "  <li>All Titan Models</li>\n",
    "  <li>All Nova Models</li>\n",
    "  <li>Claude 3 Haiku</li>\n",
    "  <li>Claude 3 Sonnet</li>\n",
    "  <li>Claude 3.5 Haiku</li>\n",
    "  <li>Claude 3.5 Sonnet</li>\n",
    "  <li>Stable Diffusion</li>\n",
    "  <li>Llama 2 13B</li>\n",
    "  <li>Llama 2 70B</li>  \n",
    "</ul>  \n",
    "\n",
    "</div>\n",
    "\n",
    "<p>Remember that you can modify model access at any point of time, but if you are running this workshop during a <b>Serverless Day event</b>, it is likely you have limited access to third-party models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec817829-c843-48ec-9f69-a28bb0daebe2",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Run the cells in this section to install the required packages for the notebooks in this workshop. ⚠️ You might encounter pip dependency warnings or conflicts during installation—these can be safely ignored for the purpose of this workshop. ⚠️\n",
    "\n",
    "_NOTE:_ The warnings about pip's dependency resolver not handling all installed packages correctly are expected and do not impact the functionality of the notebook. Proceed with the workshop without concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ca3146-5294-4700-9b3c-5089ba03c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --no-build-isolation --force-reinstall -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-dotenv",
   "metadata": {},
   "source": [
    "## Create the boto3 Client\n",
    "\n",
    "Interaction with the Bedrock API is performed via the AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html).\n",
    "\n",
    "### Use Different Clients\n",
    "The `boto3` library provides distinct clients for Amazon Bedrock to perform various operations:\n",
    "- Actions like [`InvokeModel`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [`InvokeModelWithResponseStream`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) are handled via the **Amazon Bedrock Runtime** client.\n",
    "- Operations like [ListFoundationModels](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html) are supported by the **Amazon Bedrock Client**.\n",
    "\n",
    "### Load Credentials from `.env`\n",
    "\n",
    "In this notebook, we use a `.env` file to store and load AWS credentials. Use the `dotenv` package to load and access environment variables from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aba068-7d0b-46b0-b45b-d28706dc71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import botocore.exceptions\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "aws_region = \"us-east-1\"\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "bedrock_management_client = boto3.client('bedrock', region_name=aws_region)\n",
    "bedrock_agent_client = boto3.client('bedrock-agent', region_name=aws_region)\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=aws_region)\n",
    "cloudformation_client = boto3.client('cloudformation', region_name=aws_region)\n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b609282-9cfc-48fe-9d8e-1727b8e57134",
   "metadata": {},
   "source": [
    "#### Validate the connection\n",
    "\n",
    "We can check the client works by trying out the `list_foundation_models()` method, which will tell us all the models available for us to use \n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "for model in boto3_bedrock.list_foundation_models().get('modelSummaries', []):\n",
    "    print(model.get('modelId', 'Unknown Model ID'))\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb19627-ab1c-4c1a-ac64-376a17e2d34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e7b1852-ecc2-4a95-a93b-7204c8a798b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `InvokeModel` body and output\n",
    "\n",
    "The `invoke_model()` method of the Amazon Bedrock runtime client (`InvokeModel` API) will be the primary method we use for most of our Text Generation and Processing tasks - whichever model we're using.\n",
    "\n",
    "Although the method is shared, the format of input and output varies depending on the foundation model used - as described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacb6e9-1776-4c50-bc15-ab54269ee612",
   "metadata": {},
   "source": [
    "### Amazon Titan Large and Premier\n",
    "\n",
    "#### Input\n",
    "```json\n",
    "{   \n",
    "    \"inputText\": \"<prompt>\",\n",
    "    \"textGenerationConfig\" : { \n",
    "        \"maxTokenCount\": 512,\n",
    "        \"stopSequences\": [],\n",
    "        \"temperature\": 0.1,  \n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"inputTextTokenCount\": 613,\n",
    "    \"results\": [{\n",
    "        \"tokenCount\": 219,\n",
    "        \"outputText\": \"<output>\"\n",
    "    }]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46008192-a339-4a38-b078-d4306aedff67",
   "metadata": {},
   "source": [
    "### Anthropic Claude\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"\\n\\nHuman:<prompt>\\n\\nAnswer:\",\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"completion\": \"<output>\",\n",
    "    \"stop_reason\": \"stop_sequence\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb28d15-4e70-43b6-bdd0-c23caa1f1e3a",
   "metadata": {},
   "source": [
    "### Stability AI Stable Diffusion XL\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"text_prompts\": [\n",
    "        {\"text\": \"this is where you place your input text\"}\n",
    "    ],\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": 0,\n",
    "    \"steps\": 50\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{ \n",
    "    \"result\": \"success\", \n",
    "    \"artifacts\": [\n",
    "        {\n",
    "            \"seed\": 123, \n",
    "            \"base64\": \"<image in base64>\",\n",
    "            \"finishReason\": \"SUCCESS\"\n",
    "        },\n",
    "        //...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c00ebd-43cd-4a70-86d6-87342783332d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common inference parameter definitions\n",
    "\n",
    "### Randomness and Diversity\n",
    "\n",
    "Foundation models generally support the following parameters to control randomness and diversity in the \n",
    "response.\n",
    "\n",
    "**Temperature** – Large language models use probability to construct the words in a sequence. For any \n",
    "given next word, there is a probability distribution of options for the next word in the sequence. When \n",
    "you set the temperature closer to zero, the model tends to select the higher-probability words. When \n",
    "you set the temperature further away from zero, the model may select a lower-probability word. \n",
    "\n",
    "This makes the output more deterministic and often leads to repetitive or predictable responses, as the model consistently chooses the \"most likely\" option. On the other hand, when you set the temperature further away from zero, the model is more likely to select lower-probability words. This introduces more variety and creativity into the output, resulting in responses that are less predictable and more diverse.\n",
    "\n",
    "In technical terms, the temperature modulates the probability density function for the next tokens, \n",
    "implementing the temperature sampling technique. This parameter can deepen or flatten the density \n",
    "function curve. A lower value results in a steeper curve with more deterministic responses, and a higher \n",
    "value results in a flatter curve with more random responses.\n",
    "\n",
    "![images/Capture-2024-12-09-180845.png](<images/Capture-2024-12-09-180845.png>)\n",
    "\n",
    "**Top K** – Temperature defines the probability distribution of potential words, and Top K defines the cut \n",
    "off where the model no longer selects the words. For example, if K=50, the model selects from 50 of the \n",
    "most probable words that could be next in a given sequence. This reduces the probability that an unusual \n",
    "word gets selected next in a sequence.\n",
    "In technical terms, Top K is the number of the highest-probability vocabulary tokens to keep for Top-\n",
    "K-filtering - This limits the distribution of probable tokens, so the model chooses one of the highest-\n",
    "probability tokens.\n",
    "\n",
    "**Top P** – Top P defines a cut off based on the sum of probabilities of the potential choices. If you set Top \n",
    "P below 1.0, the model considers the most probable options and ignores less probable ones. Top P is \n",
    "similar to Top K, but instead of capping the number of choices, it caps choices based on the sum of their \n",
    "probabilities.\n",
    "For the example prompt \"I hear the hoof beats of ,\" you may want the model to provide \"horses,\" \n",
    "\"zebras\" or \"unicorns\" as the next word. If you set the temperature to its maximum, without capping \n",
    "Top K or Top P, you increase the probability of getting unusual results such as \"unicorns.\" If you set the \n",
    "temperature to 0, you increase the probability of \"horses.\" If you set a high temperature and set Top K or \n",
    "Top P to the maximum, you increase the probability of \"horses\" or \"zebras,\" and decrease the probability \n",
    "of \"unicorns.\"\n",
    "\n",
    "![images/Capture-2024-12-09-180610.png](<images/Capture-2024-12-09-180610.png>)\n",
    "\n",
    "### Length\n",
    "\n",
    "The following parameters control the length of the generated response.\n",
    "\n",
    "**Response length** – Configures the minimum and maximum number of tokens to use in the generated \n",
    "response.\n",
    "\n",
    "**Length penalty** – Length penalty optimizes the model to be more concise in its output by penalizing \n",
    "longer responses. Length penalty differs from response length as the response length is a hard cut off for \n",
    "the minimum or maximum response length.\n",
    "\n",
    "In technical terms, the length penalty penalizes the model exponentially for lengthy responses. 0.0 \n",
    "means no penalty. Set a value less than 0.0 for the model to generate longer sequences, or set a value \n",
    "greater than 0.0 for the model to produce shorter sequences.\n",
    "\n",
    "### Repetitions\n",
    "\n",
    "The following parameters help control repetition in the generated response.\n",
    "\n",
    "**Repetition penalty (presence penalty)** – Prevents repetitions of the same words (tokens) in responses. \n",
    "1.0 means no penalty. Greater than 1.0 decreases repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f4005-e33a-44e1-8d4e-08e7fd7e5671",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try out the models\n",
    "\n",
    "With some theory out of the way, let's see the models in action! Run the cells below to see basic, synchronous example invocations for each model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156064e8-a918-4383-843b-888a1dfad9a0",
   "metadata": {},
   "source": [
    "### Amazon Titan Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0490c43-f955-431f-b080-04e284155463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-tg1-large',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06334283-32dc-4ab9-89e1-e644ab216632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = agent_foundation_model_selector.value\n",
    "prompt_data = \"\"\"Command: Write me a blog about making strong business decisions as a leader.\n",
    "\n",
    "Blog:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865252e8-fe1c-4de9-9086-3b1da3846ee0",
   "metadata": {},
   "source": [
    "Next, we will construct the body with the `prompt_data` above, and add a optional parameters like `topP` and `temperature`:\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"topP\": 0.95,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee40ce-32a5-4382-bb9c-bcfe9776d6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8efc857c-9be9-403e-8503-76d683e65231",
   "metadata": {},
   "source": [
    "The Amazon Bedrock API provides you with an API `invoke_model` which accepts the following:\n",
    "- `modelId`: This is the model ARN for the various foundation models available under Amazon Bedrock\n",
    "- `accept`: The type of input request\n",
    "- `contentType`: The content type of the output\n",
    "- `body`: A json string consisting of the prompt and the configurations\n",
    "\n",
    "Check [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html) for Available text generation model Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69601f88-3f47-4b4c-ac3e-92cb4ebbdd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(body, model_id):\n",
    "    try:\n",
    "        accept = \"application/json\"\n",
    "        content_type = \"application/json\"\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        output_text = response_body.get(\"results\")[0].get(\"outputText\")\n",
    "        return output_text\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "            print(\n",
    "                f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "                f\"To troubleshoot this issue, refer to the following resources:\\n\"\n",
    "                f\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "                f\"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\"\n",
    "            )\n",
    "        else:\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3c0c20-9fc0-4b8a-960a-c32886522e82",
   "metadata": {},
   "source": [
    "To invoke the Bedrock model, use the `invoke_bedrock_model` function with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147a923-91ec-417e-b2ef-587d1342d66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d9ac1d6-dbbc-488c-b787-82430c278351",
   "metadata": {},
   "source": [
    "### Stability Stable Diffusion XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ed815-0249-499b-9518-4a670b20ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Stability AI\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='stability.stable-diffusion-xl-v1',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13a891f-d850-426e-84d8-7b5204295998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = agent_foundation_model_selector.value\n",
    "prompt_data = \"a landscape with trees\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90904fc-228f-4f21-afb5-d3ac09fea227",
   "metadata": {},
   "source": [
    "Next, we will construct the body using the `prompt_data` variable and include optional parameters such as `cfg_scale`, `seed`, and `steps`:\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "body = json.dumps({\n",
    "    \"text_prompts\": [{\"text\": prompt_data}],\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": 20,\n",
    "    \"steps\": 50\n",
    "})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c036f-a838-4118-b8c3-ea535c01a4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d31e913a-bda3-4f88-b521-acfe71dd0827",
   "metadata": {},
   "source": [
    "The `invoke_model` API in Amazon Bedrock requires parameters such as `modelId`, `accept`, `contentType`, and a `body` JSON with the prompt and configurations. For available model IDs, refer to the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18aa771f-7ba7-4779-95b5-19fbd4baae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_with_artifact(body, model_id):\n",
    "    try:\n",
    "        accept = \"application/json\"\n",
    "        content_type = \"application/json\"\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        result_text = response_body.get(\"result\", \"No result found\")\n",
    "        artifact_base64 = response_body.get(\"artifacts\", [{}])[0].get(\"base64\", \"\")\n",
    "\n",
    "        print(\"Result:\")\n",
    "        print(result_text)\n",
    "        print(f\"Base64 Snippet: {artifact_base64[:80]}...\")\n",
    "\n",
    "        return response_body\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\n",
    "            print(\n",
    "                f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "                f\"To troubleshoot this issue, refer to the following resources:\\n\"\n",
    "                f\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "                f\"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\"\n",
    "            )\n",
    "        else:\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c7d8f0-b999-4a88-8133-b5378519c311",
   "metadata": {},
   "source": [
    "To invoke the Bedrock model, use the `invoke_bedrock_model` function with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c721dc-5900-4d8c-9184-d39985134da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1b48af-17d5-45b9-973d-02be120b2d51",
   "metadata": {},
   "source": [
    "**Note:** The output is a [base64 encoded](https://docs.python.org/3/library/base64.html) string of the image data. You can use any image processing library (such as [Pillow](https://pillow.readthedocs.io/en/stable/)) to decode the image as in the example below:\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "base_64_img_str = response_body.get(\"artifacts\")[0].get(\"base64\")\n",
    "image = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f514e61-95ed-4272-bc0b-dd1f0cc39f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1fe6f9f-8f48-4584-9af1-198dd3dec02f",
   "metadata": {},
   "source": [
    "## Generate streaming output\n",
    "\n",
    "For large language models, it can take noticeable time to generate long output sequences. Rather than waiting for the entire response to be available, latency-sensitive applications may like to **stream** the response to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce4007-2d8e-4708-acaa-8cb6365385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-text-premier-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "698d7a42-e35f-4e14-b842-5e1cf852748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = agent_foundation_model_selector.value\n",
    "prompt_data = \"\"\"Command: Write me a blog about making strong business decisions as a leader.\n",
    "\n",
    "Blog:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97e3cb-5bd1-4ffc-9ff0-d69281b7d049",
   "metadata": {},
   "source": [
    "Run the code below to see how you can achieve this with Bedrock's `invoke_model_with_response_stream()` method - returning the response body in separate chunks.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "body = json.dumps({\"inputText\": prompt_data})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a92ae7-522b-46f8-9573-6c50bf0f43fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18cbfc71-a570-4a0b-8a63-a454d9a13a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown, Markdown\n",
    "\n",
    "def invoke_amazon_model_with_streaming(body, model_id):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime_client.invoke_model_with_response_stream(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        stream = response.get(\"body\")\n",
    "\n",
    "        if stream:\n",
    "            print(\"Generated Content:\\n\")\n",
    "            for event in stream:\n",
    "                chunk = event.get(\"chunk\")\n",
    "                if chunk:\n",
    "                    chunk_data = json.loads(chunk.get(\"bytes\").decode())\n",
    "                    text = chunk_data.get(\"outputText\", \"\")\n",
    "                    display_markdown(Markdown(print(text, end='')))\n",
    "        else:\n",
    "            print(\"No response stream received.\")\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\n",
    "            print(\n",
    "                f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "                f\"To troubleshoot this issue, refer to the following resources:\\n\"\n",
    "                f\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "                f\"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\"\n",
    "            )\n",
    "        else:\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d1c8e9-6722-41cf-bd18-913375203c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4873836-e14f-4a80-b281-1c6d91ee753a",
   "metadata": {},
   "source": [
    "### Anthropic Claude (messages API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d408ec-c4a1-475c-b159-466d4959464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ac2d9459-e544-4c51-bd0d-395e89109781",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = agent_foundation_model_selector.value\n",
    "prompt_data = \"\"\"Human: Write me 500 word paragraph about making strong business decisions as a leader.\n",
    "\n",
    "Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec85f9b-ffb0-43e4-9c5e-b57282c600c2",
   "metadata": {},
   "source": [
    "Run the code below to see how you can achieve this with Bedrock's `invoke_model_with_response_stream()` method - returning the response body in separate chunks.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": int(500 / 0.75),\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt_data\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64155f40-f441-4579-a289-fa9ac1285ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc2b3501-b7e8-43db-ab51-98873e643773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown, Markdown\n",
    "\n",
    "def invoke_anthropic_model_with_streaming(body, model_id):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime_client.invoke_model_with_response_stream(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        stream = response.get(\"body\")\n",
    "\n",
    "        if stream:\n",
    "            print(\"Generated Content:\\n\")\n",
    "            for event in stream:\n",
    "                chunk = event.get(\"chunk\")\n",
    "                if chunk:\n",
    "                    chunk_obj = json.loads(chunk.get(\"bytes\").decode())\n",
    "                    delta_obj = chunk_obj.get(\"delta\")\n",
    "                    if delta_obj:\n",
    "                        text = delta_obj.get(\"text\")\n",
    "                        if text:\n",
    "                            display_markdown(Markdown(print(text, end='')))\n",
    "        else:\n",
    "            print(\"No response stream received.\")\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\n",
    "            print(\n",
    "                f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "                f\"To troubleshoot this issue, refer to the following resources:\\n\"\n",
    "                f\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "                f\"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\"\n",
    "            )\n",
    "        else:\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700ee6c-ddb8-495b-912f-13f8e69aeffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f055742e-21bd-42ec-8ec7-f9d95a1665b8",
   "metadata": {},
   "source": [
    "### Amazon Nova (messages API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d10730-fe22-4332-9987-6f21b984f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.nova-pro-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4e3baab2-bbb6-4894-9bee-d6d58bca92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_nova_model(body, model_id, use_streaming=False):\n",
    "    try:\n",
    "        print(\"Generated Content:\\n\")\n",
    "        if use_streaming:\n",
    "            response = bedrock_runtime_client.invoke_model_with_response_stream(\n",
    "                modelId=model_id, body=body\n",
    "            )\n",
    "            stream = response.get(\"body\")\n",
    "            chunk_count = 0\n",
    "            time_to_first_token = None\n",
    "\n",
    "            if stream:\n",
    "                for event in stream:\n",
    "                    chunk = event.get(\"chunk\")\n",
    "                    if chunk:\n",
    "                        chunk_obj = json.loads(chunk.get(\"bytes\").decode())\n",
    "                        delta_obj = chunk_obj.get(\"contentBlockDelta\")\n",
    "                        if delta_obj:\n",
    "                            text = delta_obj.get(\"delta\", {}).get(\"text\", \"\")\n",
    "                            if text:\n",
    "                                display_markdown(Markdown(print(text, end='')))\n",
    "            return None\n",
    "        else:\n",
    "            response = bedrock_runtime_client.invoke_model(\n",
    "                modelId=model_id, body=body\n",
    "            )\n",
    "            request_id = response[\"ResponseMetadata\"][\"RequestId\"]\n",
    "            model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "            content_text = model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "            print(content_text)\n",
    "            return model_response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9818c3d-538e-4d74-9007-c8ae1696b3a9",
   "metadata": {},
   "source": [
    "#### Example API request\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"modelId\": \"string\", \n",
    "  \"contentType\": \"string\", \n",
    "  \"accept\": \"string\", \n",
    "  \"body\": {\n",
    "    \"system\": [\n",
    "      {\n",
    "        \"text\": \"string\"\n",
    "      }\n",
    "    ],\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"string\", \n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"string\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"inferenceConfig\": {\n",
    "      \"max_new_tokens\": \"integer\",\n",
    "      \"top_p\": \"float\",\n",
    "      \"top_k\": \"integer\",\n",
    "      \"temperature\": \"float\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9b8c6d37-fa49-49b5-b899-ebd98dd31cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = agent_foundation_model_selector.value\n",
    "system_promt = \"You should respond to all messages in french\"\n",
    "user_request = \"Write me 500 word paragraph about making strong business decisions as a leader.\"\n",
    "inf_params = {\"max_new_tokens\": 300, \"top_p\": 0.9, \"top_k\": 20, \"temperature\": 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4038f90-c146-4842-a0cf-46b9e00ba029",
   "metadata": {},
   "source": [
    "### Create a JSON Request Body\n",
    "\n",
    "The following example demonstrates how to construct a JSON request body for your use case. It includes placeholders for system prompts, user requests, and inference configuration parameters:\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "body = json.dumps({\n",
    "    \"system\": [\n",
    "        {\"text\": \"<your-system-prompt-here>\"}\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"text\": \"<your-user-request-here>\"}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": inf_params\n",
    "})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06defab8-9a38-427e-8f82-5fe4e82502f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b6eae8-cdbd-41fe-b8d7-6deabc632cf0",
   "metadata": {},
   "source": [
    "## Generate embeddings\n",
    "\n",
    "Use text embeddings to convert text into meaningful vector representations. You input a body of text \n",
    "and the output is a (1 x n) vector, where n is the number of dimensions in the embedding space. You can use embedding vectors for a wide variety of applications. \n",
    "Bedrock currently offers Titan Embeddings for text embedding that supports text similarity (finding the \n",
    "semantic similarity between bodies of text) and text retrieval (such as search).\n",
    "\n",
    "At the time of writing you can use `amazon.titan-embed-text-v1` as embedding model via the API. The input text size is 8192 tokens and the output vector length is 1536.\n",
    "\n",
    "To use a text embeddings model, use the InvokeModel API operation or the Python SDK.\n",
    "Use InvokeModel to retrieve the vector representation of the input text from the specified model.\n",
    "\n",
    "\n",
    "\n",
    "#### Input\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"inputText\": \"<text>\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"embedding\": []\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e3173-b8e0-4df3-a51c-3e0c05d0e5cc",
   "metadata": {},
   "source": [
    "Let's see how to generate embeddings of some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ec4f9-c7a6-4b92-b92c-1bd371123247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byOutputModality=\"EMBEDDING\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='amazon.titan-embed-g1-text-02',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b4beb9-6c44-41fa-839a-e0cc8eb4ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = agent_foundation_model_selector.value\n",
    "prompt_data = \"Amazon Bedrock supports foundation models from industry-leading providers such as \\\n",
    "AI21 Labs, Anthropic, Stability AI, and Amazon. Choose the model that is best suited to achieving \\\n",
    "your unique goals.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "44550c22-4c77-4d39-8b5c-d34d6dfa428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_embedding(body, model_id):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=accept,\n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        embedding = response_body.get(\"embedding\")\n",
    "\n",
    "        if embedding:\n",
    "            print(f\"The embedding vector has {len(embedding)} values\")\n",
    "            print(f\"Preview: {embedding[:3]} ... {embedding[-3:]}\")\n",
    "            return {\"embedding\": embedding, \"message\": \"Embedding retrieved successfully.\"}\n",
    "        else:\n",
    "            print(\"No embedding found in the response.\")\n",
    "            return {\"embedding\": None, \"message\": \"No embedding found in the response.\"}\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "            message = (\n",
    "                f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "                f\"To troubleshoot this issue, refer to the following resources:\\n\"\n",
    "                f\"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot-access-denied.html\\n\"\n",
    "                f\"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\"\n",
    "            )\n",
    "            print(message)\n",
    "            return {\"embedding\": None, \"message\": message}\n",
    "        else:\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20a04a-b953-4698-8416-15a0aa57f920",
   "metadata": {},
   "source": [
    "### Create a JSON Request Body\n",
    "\n",
    "The following example demonstrates how to construct a JSON request body for your use case. \n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "body = json.dumps({\"inputText\": prompt_data})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b9e94-1c12-496a-9b34-e90328295685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885614d7-ae58-436e-9ad0-b144d827207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f09d5-6478-44cd-879b-f219555b98a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29dcf605-09fc-4f8e-8879-a90f37d9201b",
   "metadata": {},
   "source": [
    "## Getting Started with the Converse API in Amazon Bedrock\n",
    "\n",
    "The Converse API (or ConverseStream API) in Amazon Bedrock provides a unified structured text interface for invoking Bedrock LLMs. It simplifies interactions with various model providers by using a universal syntax and message-structured prompts.\n",
    "\n",
    "As a continuation of the previous setup, let's proceed to explore how to use the Converse API for structured interactions with Bedrock models. If you haven't already installed or updated boto3, ensure it's set up before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621cc08-5215-4fd3-b633-607cee52193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_IDS = [\n",
    "    \"amazon.titan-tg1-large\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615d5f3-d236-4969-80ad-58006731c322",
   "metadata": {},
   "source": [
    "We are now ready to configure the Converse API action in Amazon Bedrock. This API uses a consistent syntax for all supported models, including messages-formatted prompts and inference parameters. Additionally, the output format remains the same regardless of the model being used.\n",
    "\n",
    "Optionally, you can include model-specific request fields that are unique to certain providers, allowing for greater flexibility. For more details, refer to the Bedrock Converse API documentation.\n",
    "\n",
    "### One-Shot Invocations with Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c2c50a1f-ff3b-44d2-9190-d59aa2be7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(client, model_id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Invokes a Bedrock model using the Converse API with the provided parameters.\n",
    "\n",
    "    Args:\n",
    "        client: The Bedrock client instance.\n",
    "        model_id: The ID of the model to invoke.\n",
    "        prompt: The input prompt for the model.\n",
    "        max_tokens: The maximum number of tokens to generate (default is 2000).\n",
    "        temperature: Controls the randomness of responses (default is 0).\n",
    "        top_p: Controls the diversity of responses (default is 0.9).\n",
    "\n",
    "    Returns:\n",
    "        str: The result text including latency and token usage, or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"text\": prompt}\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Model invocation error: {e}\")\n",
    "        return \"Model invocation error\"\n",
    "\n",
    "    try:\n",
    "        result_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        latency = response[\"metrics\"][\"latencyMs\"]\n",
    "        input_tokens = response[\"usage\"][\"inputTokens\"]\n",
    "        output_tokens = response[\"usage\"][\"outputTokens\"]\n",
    "\n",
    "        result = (\n",
    "            f\"{result_text}\\n\"\n",
    "            f\"--- Latency: {latency} ms - Input tokens: {input_tokens} - Output tokens: {output_tokens} ---\\n\"\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Output parsing error: {e}\")\n",
    "        return \"Output parsing error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145e648-d9b3-4853-afc6-c1520ea4e600",
   "metadata": {},
   "source": [
    "Now, let's test our model invocation.\n",
    "\n",
    "Use the same prompt to evaluate the capabilities of all text models available in Bedrock at the time of writing.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "prompt = \"What is the capital of Italy?\"\n",
    "print(f\"### Prompt:\\n\\n{prompt}\\n\")\n",
    "\n",
    "for model_id in MODEL_IDS:\n",
    "    print(f\"\\n\\n### Testing Model: {model_id}\\n\")\n",
    "    response = invoke_bedrock_model(bedrock_runtime, model_id, prompt)\n",
    "    print(f\"Response: {response}{'-'*100}\\n\")\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d67f5-9e77-44be-95dd-b704c4e48907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "114955a1-2901-443e-8832-3cc4a15c1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_IDS = [\n",
    "    \"amazon.titan-tg1-large\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd620a-b8b8-4dad-930d-dde808bfcf37",
   "metadata": {},
   "source": [
    "### ConverseStream for streaming invocations\n",
    "\n",
    "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d142ef5d-6767-4d3a-a75a-82938e4be190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_stream(client, model_id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Invokes a Bedrock model using the ConverseStream API for streaming responses.\n",
    "\n",
    "    Args:\n",
    "        client: The Bedrock client instance.\n",
    "        model_id: The ID of the model to invoke.\n",
    "        prompt: The input prompt for the model.\n",
    "        max_tokens: Maximum tokens to generate (default: 2000).\n",
    "        temperature: Controls response randomness (default: 0).\n",
    "        top_p: Controls response diversity (default: 0.9).\n",
    "\n",
    "    Returns:\n",
    "        None. Prints the streamed response to the console in real-time.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": prompt}]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for event in response['stream']:\n",
    "            if 'contentBlockDelta' in event:\n",
    "                chunk = event['contentBlockDelta']\n",
    "                sys.stdout.write(chunk['delta']['text'])\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError invoking model {model_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a7fcb-e8c3-4e1a-ab2a-aa8da28e5703",
   "metadata": {},
   "source": [
    "Use the `invoke_bedrock_model_stream` function to test different models.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "prompt = \"What is the capital of Italy?\"\n",
    "print(f\"### Prompt:\\n\\n{prompt}\\n\")\n",
    "\n",
    "for model_id in MODEL_IDS:\n",
    "    print(f\"\\n\\n### Testing Model: {model_id}\\n\")\n",
    "    invoke_bedrock_model_stream(bedrock_runtime, model_id, prompt)\n",
    "    print(\"\\n\" + \"-\" * 100)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c5e33-55d8-491f-bd4b-c6e07ffa88a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c8f36f-dc2e-46fc-8a23-5f2e68ea71b2",
   "metadata": {},
   "source": [
    "### **Challenge 1: Text Generation with Amazon Titan or Nova**\n",
    "\n",
    "**Objective:** Leverage the **Amazon Titan** or **Nova** foundation model to craft a creative short story based on a given prompt.\n",
    "\n",
    "**Scenario:**\n",
    "As a content creator, you aim to generate engaging content for your blog using AI. Utilize the **Amazon Titan** or **Nova** large model to create a captivating story with the following prompt:\n",
    "\n",
    "> **\"Write a short story about an adventurous fox who discovers a hidden treasure in an enchanted forest.\"**\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Use the **Amazon Titan Large** or **Amazon Nova** model to generate the story.\n",
    "2. Experiment with the following parameters to fine-tune the output:\n",
    "   - **`temperature`**: Adjust the randomness and creativity of the response.\n",
    "   - **`topP`**: Control the diversity of word choices to ensure a coherent yet imaginative narrative.\n",
    "   - **`maxTokenCount`**: Limit the length of the generated story to ensure concise storytelling (e.g., a maximum of 500 tokens).\n",
    "3. Compare outputs by varying parameter values and analyze how they influence the tone, creativity, and engagement level of the story.\n",
    "4. Document the parameter settings that resulted in the most creative and engaging version of the story.\n",
    "\n",
    "**Bonus Task:**\n",
    "\n",
    "- Use the **streaming API** (e.g., `invoke_model_with_response_stream`) to generate the story incrementally.\n",
    "- Observe the real-time output and analyze how the story develops progressively.\n",
    "\n",
    "**Deliverable:**\n",
    "\n",
    "1. **Generated Stories**: Include multiple versions of the story for comparison.\n",
    "2. **Analysis**: Document how parameter changes (e.g., higher/lower `temperature` or `topP`) affected the output.\n",
    "3. **Best Settings**: Highlight the parameter configuration that produced the most engaging result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff56ea8-ddb8-437e-bbf6-b9df727d4a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cd37d03-8432-4b34-9eda-7426fc3a0732",
   "metadata": {},
   "source": [
    "### **Challenge 2: Image Generation with Amazon Nova Canvas**\n",
    "**Objective:** Use **Amazon Nova Canvas** to generate an image from a text prompt and render it in your environment.\n",
    "\n",
    "**Scenario:**\n",
    "You are designing a cover image for your fantasy novel. Use **Amazon Nova Canvas** to create an image based on the following description:\n",
    "> \"A mystical castle surrounded by glowing blue mushrooms under a starry night sky.\"\n",
    "\n",
    "**Tasks:**\n",
    "1. Use the **Amazon Nova Canvas** tool to generate the image by making an API call. Here’s an example request structure:\n",
    "   ```json\n",
    "   {\n",
    "     \"modelId\": \"amazon.nova-canvas-v1:0\",\n",
    "     \"contentType\": \"application/json\",\n",
    "     \"accept\": \"application/json\",\n",
    "     \"body\": \"{\\\"textToImageParams\\\":{\\\"text\\\":\\\"A mystical castle surrounded by glowing blue mushrooms under a starry night sky\\\"},\\\"taskType\\\":\\\"TEXT_IMAGE\\\",\\\"imageGenerationConfig\\\":{\\\"cfgScale\\\":8,\\\"seed\\\":42,\\\"quality\\\":\\\"standard\\\",\\\"width\\\":1280,\\\"height\\\":720,\\\"numberOfImages\\\":3}}\"\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. Customize the following parameters in the API call:\n",
    "   - `text`: Modify the prompt to adjust the scene or add new elements (e.g., \"a dragon flying over the castle\").\n",
    "   - `cfgScale`: Adjust how closely the image adheres to the prompt (default: 8).\n",
    "   - `width` and `height`: Configure the dimensions to suit your design.\n",
    "   - `numberOfImages`: Specify how many variations you want.\n",
    "\n",
    "3. Decode and render the image in Python using libraries like `base64` and `Pillow` or any other preferred method.\n",
    "\n",
    "4. Save the generated image to a file and display it in the notebook for review.\n",
    "\n",
    "**Bonus:**\n",
    "Experiment with changing the `text` parameter to include new narrative elements (e.g., \"a dragon flying over the castle\") and compare the results. Discuss how **Amazon Nova Canvas** handles variations and the fidelity of generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c90e0-0350-40f6-adbd-f82351bf2752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e7bc87d-2a24-41fd-806c-993bf7804c79",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this notebook, we demonstrated how to invoke Amazon Bedrock models using the AWS Python SDK (`boto3`). We covered both one-shot and streaming invocations, including examples of using the **Converse API** for structured and consistent interactions with various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e707c2d-96a4-462b-8bf2-2b8e3c44bf26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
