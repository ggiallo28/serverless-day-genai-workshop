{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview: Exploring LLM Agents and Tools with Amazon Bedrock\n",
    "\n",
    "This notebook is a practical exploration of **Large Language Model (LLM) agents** and their integration with memory, tools, and reasoning frameworks using **Amazon Bedrock**. The activities are designed to demonstrate advanced techniques for augmenting LLMs via LangChain, a framework for building applications powered by language models.\n",
    "\n",
    "### Objectives:\n",
    "1. **Introduction to Chain of Thought (CoT) and Tree of Thoughts (ToT):**\n",
    "   - Learn how multi-step reasoning enhances LLM performance.\n",
    "   - Implement sequential reasoning using CoT techniques.\n",
    "\n",
    "2. **Memory Integration:**\n",
    "   - Understand different types of memory: sensory, short-term, and long-term.\n",
    "   - Implement long-term memory using vector databases.\n",
    "   - Use short-term memory to maintain conversational context.\n",
    "\n",
    "3. **Tool Integration:**\n",
    "   - Explore how external tools (e.g., search APIs, calculators) can extend LLM capabilities.\n",
    "   - Build and use custom tools to answer complex queries.\n",
    "\n",
    "4. **Agent Construction:**\n",
    "   - Define agents that combine reasoning, tools, and memory for advanced workflows.\n",
    "   - Implement ReAct (Reasoning and Acting) agents using Amazon Bedrock.\n",
    "\n",
    "5. **Structuring Outputs:**\n",
    "   - Learn to format outputs using structured data models (e.g., Pydantic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "import ipywidgets as widgets\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "config = load_config(\"chain_config.json\")\n",
    "load_dotenv(\".env\")\n",
    "aws_region = \"us-east-1\"\n",
    "\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "bedrock_management_client = boto3.client('bedrock', region_name=aws_region)\n",
    "bedrock_agent_client = boto3.client('bedrock-agent', region_name=aws_region)\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=aws_region)\n",
    "cloudformation_client = boto3.client('cloudformation', region_name=aws_region)\n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning\n",
    "\n",
    "![images/Capture-2024-11-23-235604.png](<images/Capture-2024-11-23-235604.png>)\n",
    "\n",
    "You may have encountered different strategies designed to enhance the performance of large language models, ranging from giving them guidance to lightheartedly \"motivating\" them. A widely recognized method is the \"chain of thought\" technique, where the model is prompted to reason through problems step by step, allowing it to identify and fix errors as it goes. This strategy has been further developed into more sophisticated approaches, such as \"chain of thought with self-consistency,\" and extended into the broader \"tree of thoughts\" framework. In this generalized approach, multiple lines of reasoning are generated, re-examined, and integrated to arrive at a more robust and accurate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-text-express-v1',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree of Thoughts\n",
    "\n",
    "The [@astropomeai tutorial](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac) on Tree of Thoughts is used as basis of this exercise but expanded with LLMOps tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_step1_prompt = PromptTemplate(template=config[\"step1\"][\"prompt\"], input_variables=[\"input\", \"perfect_factors\"])\n",
    "cot_step2_prompt = PromptTemplate(template=config[\"step2\"][\"prompt\"], input_variables=[\"solutions\"])\n",
    "cot_step3_prompt = PromptTemplate(template=config[\"step3\"][\"prompt\"], input_variables=[\"review\"])\n",
    "cot_step4_prompt = PromptTemplate(template=config[\"step4\"][\"prompt\"], input_variables=[\"deepen_thought_process\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent_foundation_model_selector.value\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatBedrock(temperature=0, model=model),\n",
    "    prompt=cot_step1_prompt,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatBedrock(temperature=0, model=model),\n",
    "    prompt=cot_step2_prompt,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatBedrock(temperature=0, model=model),\n",
    "    prompt=cot_step3_prompt,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatBedrock(temperature=0, model=model),\n",
    "    prompt=cot_step4_prompt,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"input\": \"human colonization of Mars\",\n",
    "    \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "}\n",
    "\n",
    "response = overall_chain.invoke(params)\n",
    "print(response['ranked_solutions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is now to rewrite the above logic by using the `|` operator. You can also take advantage of `StrOutputParser`, and you can import it in this way: `from langchain_core.output_parsers import StrOutputParser`.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatBedrock(temperature=0, model=model)\n",
    "\n",
    "chain1 = cot_step1_prompt | llm | {\"solutions\": StrOutputParser()}\n",
    "chain2 = cot_step2_prompt | llm | {\"review\": StrOutputParser()}\n",
    "chain3 = cot_step3_prompt | llm | {\"deepen_thought_process\": StrOutputParser()}\n",
    "chain4 = cot_step4_prompt | llm | {\"ranked_solutions\": StrOutputParser()}\n",
    "\n",
    "overall_chain = chain1 | chain2 | chain3 | chain4\n",
    "\n",
    "params = {\n",
    "    \"input\": \"human colonization of Mars\",\n",
    "    \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "}\n",
    "\n",
    "result = overall_chain.invoke(params)\n",
    "print(result['ranked_solutions'])\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct prompt overview\n",
    "\n",
    "Let's review [ReAct](https://python.langchain.com/docs/modules/agents/agent_types/react) prompt as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_agent_prompt = PromptTemplate(\n",
    "    template=config[\"react_agent\"][\"prompt\"], \n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")\n",
    "\n",
    "print(react_agent_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_utils import get_trivia_react_agent\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    temperature=0, \n",
    "    model=agent_foundation_model_selector.value\n",
    ")\n",
    "\n",
    "agent = get_trivia_react_agent(llm, react_agent_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke({\"input\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke({\"input\": \"What is the capital of Oswanda?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke({\"input\": \"What is the capital of Althera?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why does the Agent refuse to fully trust the tool in certain cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-ask with search\n",
    "\n",
    "Let's review [self-ask with search](https://python.langchain.com/docs/modules/agents/agent_types/self_ask_with_search) as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName']\n",
    "    ],\n",
    "    value='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_ask_with_search_prompt = PromptTemplate(\n",
    "    template=config[\"self_ask_with_search\"][\"prompt\"], \n",
    "    input_variables=[\"input\", \"agent_scratchpad\"]\n",
    ")\n",
    "\n",
    "print(self_ask_with_search_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to read next:\n",
    "\n",
    "**Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "\n",
    "**Chain of Hindsight** (CoH; [Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import SelfAskWithSearchChain\n",
    "from langchain.agents import AgentExecutor, create_self_ask_with_search_agent\n",
    "from langchain.tools import tool\n",
    "import logging\n",
    "\n",
    "model = agent_foundation_model_selector.value\n",
    "\n",
    "@tool(\"Intermediate Answer\")\n",
    "def simulated_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool searching for information on internet.\n",
    "    \"\"\"\n",
    "    return \"I'm sorry, I couldn't find information on that topic.\"\n",
    "\n",
    "\n",
    "simulated_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.05,\n",
    "    check_every_n_seconds=0.5,\n",
    "    max_bucket_size=5,\n",
    ")\n",
    "\n",
    "model = agent_foundation_model_selector.value\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    temperature=0,\n",
    "    model=model,\n",
    "    rate_limiter=rate_limiter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = create_self_ask_with_search_agent(\n",
    "    llm=llm,\n",
    "    tools=[simulated_search],\n",
    "    prompt=self_ask_with_search_prompt\n",
    ")\n",
    "\n",
    "self_ask_with_search_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[simulated_search],\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "question = \"How far away is the colonization of Mars?\"\n",
    "\n",
    "answer = self_ask_with_search_executor.invoke({\"input\": question})\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "![./images/memory.png](<./images/memory.png>)\n",
    "\n",
    "\n",
    "- **Sensory Memory:** This component of memory captures immediate sensory inputs, like what we see, hear, or feel. In the context of prompt engineering and AI models, a prompt serves as a transient input, similar to a momentary touch or sensation. It's the initial stimulus that triggers the model's processing.\n",
    "\n",
    "- **Short-Term Memory:** Short-term memory holds information temporarily, typically related to the ongoing task or conversation. In prompt engineering, this equates to retaining the recent chat history. This memory enables the agent to maintain context and coherence throughout the interaction, ensuring that responses align with the current dialogue.\n",
    "\n",
    "- **Long-Term Memory:** Long-term memory stores both factual knowledge and procedural instructions. In AI models, this is represented by the data used for training and fine-tuning. Additionally, long-term memory supports the operation of RAG frameworks, allowing agents to access and integrate learned information into their responses. It's like the comprehensive knowledge repository that agents draw upon to generate informed and relevant outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding long-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_community.vectorstores import DuckDB \n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byOutputModality=\"EMBEDDING\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='amazon.titan-embed-g1-text-02',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb \n",
    "\n",
    "connection = duckdb.connect(\"data.db\")\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=agent_foundation_model_selector.value, \n",
    "    region_name=aws_region\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Use `WebBaseLoader` to Load a Document:**\n",
    "   - Fetch the content of the webpage at `https://aws.amazon.com/ai/our-story` using `WebBaseLoader`.\n",
    "\n",
    "2. **Set up `RecursiveCharacterTextSplitter`:**\n",
    "   - Create an instance of `RecursiveCharacterTextSplitter` with the following parameters:\n",
    "     - `chunk_size=1000` (each chunk should have a maximum size of 1000 characters).\n",
    "     - `chunk_overlap=200` (each chunk should overlap the previous one by 200 characters).\n",
    "\n",
    "3. **Call `split_documents`:**\n",
    "   - Use the `split_documents` method from `RecursiveCharacterTextSplitter` to process the document loaded with `WebBaseLoader`.\n",
    "   \n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "url = \"https://aws.amazon.com/ai/our-story\"\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Create a Vector Store Using `from_documents`:**\n",
    "   - Use the `DuckDB.from_documents` method to initialize a vector store.\n",
    "   - Pass the following parameters:\n",
    "     - `documents`: A variable containing the preprocessed documents.\n",
    "     - `embedding`: A variable containing the embedding model (e.g., `bedrock_embeddings`).\n",
    "\n",
    "2. **Set Up a Retriever Using `as_retriever`:**\n",
    "   - Call the `as_retriever` method on the created vector store to transform it into a retriever.\n",
    "   \n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "duckdb_vectorstore = DuckDB.from_documents(\n",
    "    connection=connection,\n",
    "    documents=documents,\n",
    "    embedding=bedrock_embeddings\n",
    ")\n",
    "retriever = duckdb_vectorstore.as_retriever()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using OpenSearch as a Vector Store in LangChain\n",
    "\n",
    "LangChain supports various databases as vector stores, including OpenSearch. Below is an example of setting up OpenSearch as a vector store for use in LangChain.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the Code Example:</summary>\n",
    "    \n",
    "```python\n",
    "from opensearchpy import AWSV4SignerAuth\n",
    "from opensearchpy.connection import RequestsHttpConnection\n",
    "import boto3\n",
    "\n",
    "HOST = \"https://<your-opensearch-endpoint>.aoss.amazonaws.com\"\n",
    "INDEX_NAME = \"aws-ai-our-story\"\n",
    "INDEX_DIMENSION = 1024\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, aws_region, \"aoss\")\n",
    "\n",
    "opensearch_vectorstore = OpenSearchVectorSearch.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=bedrock_embeddings,\n",
    "    opensearch_url=HOST,\n",
    "    index_name=INDEX_NAME,\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "index_mapping = {\n",
    "    \"settings\": {\"index\": {\"knn\": True, \"knn.algo_param.ef_search\": 512}},\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector_field\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 128,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"nmslib\",\n",
    "                    \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "if not opensearch_vectorstore.client.indices.exists(index=INDEX_NAME):\n",
    "    opensearch_vectorstore.client.indices.create(index=INDEX_NAME, body=index_mapping)\n",
    "\n",
    "retriever = opensearch_vectorstore.as_retriever()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does AWS offer for Responsible AI?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding short-term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName']\n",
    "    ],\n",
    "    value='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[],\n",
    "        template=\"You are a helpful assistant\"\n",
    "    )\n",
    ")\n",
    "\n",
    "chat_history_placeholder = MessagesPlaceholder(\n",
    "    variable_name=\"chat_history\",\n",
    "    optional=True\n",
    ")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"{input}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "messages = [system_message, chat_history_placeholder, human_message]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Model Selection:**\n",
    "   - Retrieve the selected model from a UI component or variable, such as `agent_foundation_model_selector`.\n",
    "   - Assign the selected model to a variable for further use.\n",
    "\n",
    "2. **Initialize the Model:**\n",
    "   - Use the selected model to create an instance of a chat-based language model (e.g., `ChatBedrock`).\n",
    "   - Set appropriate parameters, such as `temperature`, to control the model's behavior.\n",
    "\n",
    "3. **Create a Runnable Pipeline:**\n",
    "   - Combine the `prompt` with the initialized model to create a runnable pipeline.\n",
    "   - Use the pipeline to process input through the prompt and model.\n",
    "   \n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "model = agent_foundation_model_selector.value\n",
    "model = ChatBedrock(temperature=0, model=model)\n",
    "\n",
    "runnable = prompt | model\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent_foundation_model_selector.value\n",
    "model = ChatBedrock(temperature=0, model=model)\n",
    "\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    runnable,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"<foo>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"hi! I'm bob\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"what's my name?\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the Content of a Session and Experiment with Other Session IDs\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "print(str(get_session_history(session_id)))\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools in AI Systems\n",
    "\n",
    "![./images/tools.png](<./images/tools.png>)\n",
    "\n",
    "The **earliest** implementations of tool-augmented AI architectures were introduced in 2022 by A21Labs.\n",
    "\n",
    "These tools can be categorized as either **neural**, such as deep learning models, or **symbolic**, like a mathematical calculator, currency converter, or weather API.\n",
    "\n",
    "Examples of such architectures in action include ChatGPT’s [Plugins](https://openai.com/blog/chatgpt-plugins) and the **OpenAI API** [function calling](https://platform.openai.com/docs/guides/gpt/function-calling), both of which enable LLMs to effectively leverage external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory as a retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"aws_ai_story_search\",\n",
    "    description=(\n",
    "        \"Search for information about AWS AI and its initiatives. \"\n",
    "        \"Use this tool to answer questions related to AWS AI story and related content.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Tools\n",
    "\n",
    "Discover more tools at [LangChain Integrations](https://python.langchain.com/docs/integrations/tools).\n",
    "\n",
    "LangChain provides a variety of agent types, each tailored to different scenarios. Below are some of the key agents offered:\n",
    "\n",
    "- **Zero-shot ReAct:** \n",
    "  - Operates using the ReAct framework, selecting tools based only on their descriptions.\n",
    "  - Requires clear descriptions for each tool and is highly adaptable to diverse tasks.\n",
    "\n",
    "- **Structured Input ReAct:** \n",
    "  - Designed for tools with multiple input parameters.\n",
    "  - Ideal for complex operations like web browsing, as it leverages the tools' argument schemas for structured inputs.\n",
    "\n",
    "- **OpenAI Functions:** \n",
    "  - Tailored for models optimized for function calling, such as `gpt-3.5-turbo-0613` and `gpt-4-0613`.\n",
    "  - Used in creating our first agent example earlier.\n",
    "\n",
    "- **Conversational Agent:** \n",
    "  - Focused on dialogue-based applications, using the ReAct framework for tool selection.\n",
    "  - Incorporates memory to retain context from previous interactions.\n",
    "\n",
    "- **Self-ask with Search:** \n",
    "  - Uses a single tool, \"Intermediate Answer,\" to retrieve factual information.\n",
    "  - Based on the original self-ask with search methodology.\n",
    "\n",
    "- **ReAct Document Store:** \n",
    "  - Interacts with a document store through the ReAct framework.\n",
    "  - Requires tools like \"Search\" and \"Lookup,\" closely resembling the approach used in the original ReAct paper's Wikipedia example.\n",
    "\n",
    "For further details on these agents, check out [this blog post](https://nanonets.com/blog/langchain/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" in model['modelName']\n",
    "    ],\n",
    "    value='amazon.nova-lite-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrockConverse(\n",
    "    temperature=0.0,\n",
    "    model=agent_foundation_model_selector.value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Provide the Necessary Inputs:**\n",
    "   - Use a `retriever_tool` as the tool the agent will use for retrieving information.\n",
    "   - Pass an `llm` (Language Learning Model) as the core reasoning model.\n",
    "\n",
    "2. **Set Up the Agent Type:**\n",
    "   - Use `AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION` to configure the agent for structured chat interactions.\n",
    "\n",
    "3. **Enable Verbose Output:**\n",
    "   - Set the `verbose` parameter to `True` to enable detailed logging of the agent's activities.\n",
    "\n",
    "4. **Write the Code:**\n",
    "   - Implement the code to create the agent using the `initialize_agent` function.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "agent_chain = initialize_agent(\n",
    "    [retriever_tool],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the agent by invoking it with different Nova language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_chain.invoke(\n",
    "    {\"input\": \"What services does AWS AI offer?\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_chain.invoke(\n",
    "    \"How has AWS AI influenced global AI research trends, and what are the long-term implications for industries like healthcare and finance?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools and reasoning\n",
    "\n",
    "Here is a full guide on how to create [custom tools for LangChain](https://python.langchain.com/docs/modules/agents/tools/custom_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Description: Brave Search Integration\n",
    "\n",
    "The `BraveSearch` tool integrates Brave's search engine capabilities into your application, allowing you to perform web searches programmatically.\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The tool is initialized using an API key (`api_key`).\n",
    "   - Additional search parameters, such as `count`, can be passed through `search_kwargs`.\n",
    "\n",
    "2. **Usage:**\n",
    "   - The `count` parameter in `search_kwargs` specifies the number of search results to return (in this case, 3).\n",
    "\n",
    "3. **Functionality:**\n",
    "   - Performs web searches using the Brave search engine.\n",
    "   - Provides a convenient way to retrieve relevant results for various queries.\n",
    "\n",
    "This tool is ideal for integrating web search functionality into applications that require programmatic access to search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import BraveSearch\n",
    "\n",
    "api_key= \"BSAj2ZNvwVxXtx1HzY05vfDVSNizcgi\"\n",
    "brave_tool = BraveSearch.from_api_key(api_key=api_key, search_kwargs={\"count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brave_tool.description, brave_tool.args,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brave_tool.run(tool_input=\"AWS AI services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Description: Scrape and Convert to Markdown\n",
    "\n",
    "The `scrape_and_convert_to_markdown_tool` is a custom tool that performs the following tasks:\n",
    "\n",
    "1. **Input:** Accepts a URL as a string.\n",
    "2. **Sanitization:** Ensures the URL has a valid scheme (e.g., `https://`). If not, it prepends `https://` to the input.\n",
    "3. **Web Scraping:** Fetches the webpage content using the `requests` library.\n",
    "4. **HTML Parsing:** Extracts the body of the webpage using `BeautifulSoup`.\n",
    "5. **Markdown Conversion:** Converts the HTML content to Markdown format using the `markdownify` library with ATX-style headings.\n",
    "6. **Output:** Returns the converted Markdown content or an error message if the scraping process fails.\n",
    "\n",
    "This tool is useful for retrieving and converting webpage content into a Markdown-readable format for further use in documentation, content management, or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify\n",
    "from langchain.tools import tool\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "@tool\n",
    "def scrape_and_convert_to_markdown_tool(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Scrapes the content of the given URL and converts it to Markdown format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url.strip())\n",
    "        if not parsed_url.scheme:\n",
    "            sanitized_url = f\"https://{url.strip()}\"\n",
    "        else:\n",
    "            sanitized_url = urlunparse(parsed_url)\n",
    "\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        response = requests.get(sanitized_url, headers=headers)\n",
    "        response = requests.get(sanitized_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content = soup.body\n",
    "        markdown_content = markdownify(str(content), heading_style=\"ATX\")\n",
    "\n",
    "        return markdown_content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching the URL: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_and_convert_to_markdown_tool.description, scrape_and_convert_to_markdown_tool.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scrape_and_convert_to_markdown_tool.run(tool_input=\"https://aws.amazon.com/ai/services/\")[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Write a Python function decorated with `@tool`.\n",
    "2. The tool should take a string as input.\n",
    "3. The tool should return the length of the input string.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate_string_length(input_string: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the length of the input string.\n",
    "    \"\"\"\n",
    "    return len(input_string)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_string_length.description, calculate_string_length.args,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_string_length.run(tool_input=\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Write a Python function and decorate it with `@tool`.\n",
    "2. The tool should accept a string as input.\n",
    "3. The tool should return the number of uppercase letters in the input string.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate_uppercase_tool(input_string: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the number of uppercase letters in the input string.\n",
    "    \"\"\"\n",
    "    return sum(1 for char in input_string if char.isupper())\n",
    "```\n",
    "\n",
    "</details> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_uppercase_tool.description, calculate_uppercase_tool.args,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_uppercase_tool.run(tool_input=\"A\"*4+\"b\"*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    temperature=0.0,\n",
    "    model=agent_foundation_model_selector.value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Provide the Necessary Inputs:**\n",
    "   - Create a `tools` list to define the actions the agent can perform.\n",
    "   - Use an `llm` (Language Model) as the core reasoning component.\n",
    "\n",
    "2. **Set Up the Agent Type:**\n",
    "   - Configure the agent with `AgentType.ZERO_SHOT_REACT_DESCRIPTION` for zero-shot reasoning and structured responses.\n",
    "\n",
    "3. **Enable Verbose Output:**\n",
    "   - Set the `verbose` parameter to `True` to enable detailed logging and provide insights into the agent's actions.\n",
    "\n",
    "4. **Write the Code:**\n",
    "   - Use the `initialize_agent` function to set up the agent with the specified tools, language model, and configuration.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [brave_tool, calculate_string_length, calculate_uppercase_tool, scrape_and_convert_to_markdown_tool]\n",
    "agent_chain = initialize_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in tools:\n",
    "    print(\"Tool Name:\", tool.name)\n",
    "    print(\"Tool Description:\", tool.description)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = '\\n'.join([\n",
    "    \"Find the description of AWS AI services.\",\n",
    "    \"Scrape the content of the relevant URL(s).\",\n",
    "    \"Convert the scraped content to Markdown format.\",\n",
    "    \"Calculate the length of the Markdown description.\",\n",
    "])\n",
    "\n",
    "response = agent_chain.invoke(\n",
    "    {\"input\": query}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of Activities: Building a LangChain-Powered System with Amazon Bedrock\n",
    "\n",
    "This recap provides an overview of the tools, memory systems, and configurations we implemented to create a powerful system leveraging **LangChain** and **Amazon Bedrock**. This system integrates reasoning, information retrieval, and text processing to deliver efficient and context-aware interactions.\n",
    "\n",
    "---\n",
    "\n",
    "### Tools\n",
    "\n",
    "#### 1. **Brave Search Tool**\n",
    "We integrated the **Brave Search API** to programmatically retrieve web content. The following code demonstrates how we set up the Brave Search tool:\n",
    "\n",
    "```python\n",
    "from langchain_community.tools import BraveSearch\n",
    "\n",
    "api_key = \"<your_api_key>\"\n",
    "brave_tool = BraveSearch.from_api_key(api_key=api_key, search_kwargs={\"count\": 3})\n",
    "```\n",
    "\n",
    "#### 2. **Custom Tools**\n",
    "We created several tools to enhance the system's functionality:\n",
    "\n",
    "- **Calculate String Length**:\n",
    "  A tool to compute the length of an input string:\n",
    "  ```python\n",
    "  from langchain.tools import tool\n",
    "\n",
    "  @tool\n",
    "  def calculate_length_tool(input_string: str) -> int:\n",
    "      \"\"\"Calculates the length of the input string.\"\"\"\n",
    "      return len(input_string)\n",
    "  ```\n",
    "\n",
    "- **Count Uppercase Letters**:\n",
    "  A tool to count the number of uppercase letters in a string:\n",
    "  ```python\n",
    "  @tool\n",
    "  def calculate_uppercase_tool(input_string: str) -> int:\n",
    "      \"\"\"Counts the number of uppercase characters in the input string.\"\"\"\n",
    "      return sum(1 for char in input_string if char.isupper())\n",
    "  ```\n",
    "\n",
    "- **Scrape and Convert to Markdown**:\n",
    "  A tool to scrape content from a webpage and convert it to Markdown format:\n",
    "  ```python\n",
    "  import requests\n",
    "  from bs4 import BeautifulSoup\n",
    "  from markdownify import markdownify\n",
    "  from langchain.tools import tool\n",
    "  from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "  @tool\n",
    "  def scrape_and_convert_to_markdown_tool(url: str) -> str:\n",
    "      \"\"\"\n",
    "      Scrapes the content of the given URL and converts it to Markdown format.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          parsed_url = urlparse(url.strip())\n",
    "          if not parsed_url.scheme:\n",
    "              sanitized_url = f\"https://{url.strip()}\"\n",
    "          else:\n",
    "              sanitized_url = urlunparse(parsed_url)\n",
    "\n",
    "          headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "          response = requests.get(sanitized_url, headers=headers)\n",
    "          response.raise_for_status()\n",
    "          soup = BeautifulSoup(response.text, 'html.parser')\n",
    "          content = soup.body\n",
    "          if content:\n",
    "              markdown_content = markdownify(str(content), heading_style=\"ATX\")\n",
    "              return markdown_content\n",
    "          else:\n",
    "              return \"No content found in the body of the page.\"\n",
    "      except requests.exceptions.RequestException as e:\n",
    "          return f\"Error fetching the URL: {e}\"\n",
    "      except Exception as e:\n",
    "          return f\"An error occurred: {e}\"\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### Memory Systems\n",
    "\n",
    "#### 1. **Long-Term Memory**\n",
    "We implemented long-term memory using **DuckDB** as a vector database to store and retrieve document embeddings. This allows the system to retain and query relevant knowledge efficiently.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import DuckDB\n",
    "\n",
    "# Split content into chunks\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "# Create DuckDB vector store\n",
    "duckdb_vectorstore = DuckDB.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=bedrock_embeddings,\n",
    "    connection=connection\n",
    ")\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = duckdb_vectorstore.as_retriever()\n",
    "\n",
    "# Create retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"aws_ai_story_search\",\n",
    "    description=\"Searches for information about AWS AI services and initiatives.\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **Short-Term Memory**\n",
    "To manage short-term memory, we used **ChatMessageHistory** to store and retrieve conversation context during interactions:\n",
    "\n",
    "```python\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "message_history = ChatMessageHistory()\n",
    "```\n",
    "\n",
    "#### 3. **Sensory Memory**\n",
    "We used **Amazon Bedrock** for real-time prompt-based interaction, leveraging predefined templates for context-aware responses:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain import hub\n",
    "\n",
    "# Load a predefined prompt template\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# Initialize the Bedrock language model\n",
    "model = ChatBedrock(temperature=0.7)\n",
    "runnable = prompt | model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All tools together\n",
    "\n",
    "![images/agent.png](<images/agent.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an agent with tools and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent_foundation_model_selector.value\n",
    "llm = ChatBedrock(model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DuckDB\n",
    "import duckdb\n",
    "\n",
    "\n",
    "con = duckdb.connect(\"data.db\")\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\", \n",
    "    region_name=aws_region\n",
    ")\n",
    "\n",
    "def get_retriever(con, embed, url, chunk_size=500, chunk_overlap=100):\n",
    "    loader = WebBaseLoader(url)\n",
    "\n",
    "    documents = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    ).split_documents(loader.load())\n",
    "    \n",
    "    duckdb_vectorstore = DuckDB.from_documents(\n",
    "        connection=con,\n",
    "        documents=documents,\n",
    "        embedding=embed\n",
    "    )\n",
    "\n",
    "    return duckdb_vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=get_retriever(\n",
    "        con=con, \n",
    "        embed=bedrock_embeddings,\n",
    "        url=\"https://aws.amazon.com/ai/our-story\"\n",
    "    ),\n",
    "    name=\"aws_ai_story_search\",\n",
    "    description=(\n",
    "        \"Search for information about AWS AI and its initiatives. \"\n",
    "        \"Use this tool to answer questions related to AWS AI story and related content.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import BraveSearch\n",
    "\n",
    "api_key = \"BSAj2ZNvwVxXtx1HzY05vfDVSNizcgi\"\n",
    "\n",
    "brave_tool = BraveSearch.from_api_key(\n",
    "    api_key=api_key, \n",
    "    search_kwargs={\"count\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def scrape_and_convert_to_markdown_tool(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Scrapes the content of the given URL and converts it to Markdown format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url.strip())\n",
    "        if not parsed_url.scheme:\n",
    "            sanitized_url = f\"https://{url.strip()}\"\n",
    "        else:\n",
    "            sanitized_url = urlunparse(parsed_url)\n",
    "\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        response = requests.get(sanitized_url, headers=headers)\n",
    "        response = requests.get(sanitized_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content = soup.body\n",
    "        markdown_content = markdownify(str(content), heading_style=\"ATX\")\n",
    "\n",
    "        return markdown_content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching the URL: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate_uppercase_tool(input_text: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of uppercase letters in the provided input string.\n",
    "\n",
    "    Args:\n",
    "        input_text: The string to analyze.\n",
    "\n",
    "    Returns:\n",
    "        The count of uppercase characters in the input string.\n",
    "    \"\"\"\n",
    "    return sum(1 for c in input_text if c.isupper())\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate_length_tool(input_text: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the total number of characters in the provided input string.\n",
    "\n",
    "    Args:\n",
    "        input_text: The string to analyze.\n",
    "\n",
    "    Returns:\n",
    "        The length of the input string, including spaces and special characters.\n",
    "    \"\"\"\n",
    "    return len(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    retriever_tool, \n",
    "    brave_tool, \n",
    "    scrape_and_convert_to_markdown_tool, \n",
    "    calculate_length_tool, \n",
    "    calculate_uppercase_tool\n",
    "]\n",
    "\n",
    "for tool in tools:\n",
    "    des = tool.description.split(\"\\n\")\n",
    "    print(f\"{tool.name}: {des[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "config = load_config(\"chain_config.json\")\n",
    "\n",
    "react_agent_with_chat_history_prompt = PromptTemplate(\n",
    "    template=config[\"react_agent_with_chat_history\"][\"prompt\"], \n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")\n",
    "\n",
    "print(react_agent_with_chat_history_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "1. **Create a React Agent:**\n",
    "   - Use the `create_react_agent` function to initialize a React agent.\n",
    "   - Pass the `llm` (language model), `tools`, and `react_agent_with_chat_history_prompt` as parameters.\n",
    "\n",
    "2. **Set Up the Agent Executor:**\n",
    "   - Create an `AgentExecutor` to manage the agent's execution.\n",
    "   - Configure the `AgentExecutor` with:\n",
    "     - The React agent (`agent`).\n",
    "     - A list of tools (`tools`).\n",
    "     - `verbose=True` for detailed logging.\n",
    "     - `handle_parsing_errors=True` to gracefully handle any parsing issues during execution.\n",
    "\n",
    "3. **Enable Chat History:**\n",
    "   - Wrap the `AgentExecutor` in a `RunnableWithMessageHistory` to include session history in the interaction.\n",
    "   - Configure the keys for managing input, history, and output:\n",
    "     - `input_messages_key=\"input\"`: Key for the input messages.\n",
    "     - `history_messages_key=\"chat_history\"`: Key for storing session history.\n",
    "     - `output_messages_key=\"output\"`: Key for the agent's output messages.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Solution:\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_react_agent\n",
    "\n",
    "# Step 1: Create a React Agent\n",
    "agent = create_react_agent(llm, tools, react_agent_with_chat_history_prompt)\n",
    "\n",
    "# Step 2: Set up the Agent Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Step 3: Enable Chat History\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"output\"\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"What is the current population of Italy?\"\n",
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": f\"Hi! I'm bob. {instruction}\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What's my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"In all the next questions I will ask you, you need to call me with my name before answering, is it ok for you?\"\n",
    "response = agent_with_chat_history.invoke(\n",
    "    {\"input\": instruction},\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"How many under 18 in Italy?\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"What are the AI services provided by AWS? Does AWS have a region in Italy?\"\n",
    "    }, \n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the weather today in Italy?\"\n",
    "    }, \n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"Make a quick summary of what we have discussed today\"\n",
    "    }, \n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"Calculate the length of this summary and the uppercase characters of this summary\"\n",
    "    }, \n",
    "    config={\"configurable\": {\"session_id\": session_id}},\n",
    ")\n",
    "\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structuring Outputs\n",
    "\n",
    "Sometimes, the output of a language model needs to serve as input for another program. In such cases, it's important to follow a specific structure, such as creating a JSON object that the target program can interpret. In this section, we will walk through an example of how to structure the output of a language model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_aws import ChatBedrock\n",
    "from pydantic import BaseModel, Field\n",
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesiredStructure(BaseModel):\n",
    "    question: str = Field(description=\"The question asked.\")\n",
    "    numerical_answer: int = Field(description=\"The number extracted from the answer, text excluded.\")\n",
    "    text_answer: str = Field(description=\"The text part of the answer, numbers excluded.\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=DesiredStructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent_foundation_model_selector.value\n",
    "llm = ChatBedrock(model=model, temperature=0)\n",
    "\n",
    "prompt_template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n",
    "query = \"Find the description of Neurons Lab services and calculate the length of this description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. **Set Up the Prompt:**\n",
    "   - Use `PromptTemplate` to create a prompt.\n",
    "   - Pass the `prompt_template` variable as the template.\n",
    "   - Use `parser.get_format_instructions()` to provide the `format_instructions` as a partial variable.\n",
    "\n",
    "2. **Combine Prompt and Model:**\n",
    "   - Chain the prompt with the `llm` (language model) to create a pipeline.\n",
    "\n",
    "3. **Invoke the Model:**\n",
    "   - Use the combined prompt and model to generate output by passing `query` as input.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt_and_model = prompt | llm\n",
    "\n",
    "output = prompt_and_model.invoke({\n",
    "    \"query\": query\n",
    "})\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.loads(output.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next stes: evaluation and benchmarks https://python.langchain.com/docs/langsmith/walkthrough\n",
    "\n",
    "How to implement Q&A https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Exploring the Inner Workings of an Agent in LangChain\n",
    "\n",
    "This code introduces a custom **Trivia Agent** built on **LangChain** to demonstrate the intricate logic and processes involved in agent execution. The goal is to unravel the mechanics behind how an agent interprets queries, decides on actions, and uses tools to provide intelligent responses.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose of the Code\n",
    "\n",
    "The code implements a step-by-step execution flow for an agent tasked with answering trivia questions. It showcases:\n",
    "\n",
    "1. **Agent Logic and Prompting:**\n",
    "   - How the agent uses a carefully structured prompt to contextualize its decisions.\n",
    "   - Dynamic tool descriptions and scratchpads (intermediate state tracking) are passed to the prompt for efficient reasoning.\n",
    "\n",
    "2. **Tool Utilization:**\n",
    "   - Tools are integrated for specific tasks, such as answering trivia or providing factual data.\n",
    "   - The agent selects the appropriate tool dynamically based on its reasoning.\n",
    "\n",
    "3. **Iterative Reasoning:**\n",
    "   - The agent evaluates the response at each step to determine whether it has reached a conclusion or needs to perform further actions.\n",
    "\n",
    "4. **Transparency and Debugging:**\n",
    "   - Verbose output provides insights into every stage of the agent’s execution, aiding in understanding and debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components of the Code\n",
    "\n",
    "#### **Prompt Template**\n",
    "The prompt template defines the structure of the input passed to the language model. It includes information about tools, their names, the user’s input, and the agent's scratchpad:\n",
    "\n",
    "#### **Core Methods**\n",
    "- **Action Parsing (`get_actions`)**: Extracts actionable instructions from the language model’s response.\n",
    "- **Thought Extraction (`get_last_thought`)**: Identifies and extracts the model's reasoning from its response.\n",
    "- **Action Execution (`execute_action`)**: Executes the determined action using the selected tool.\n",
    "- **Scratchpad Construction (`construct_scratchpad`)**: Builds a log of intermediate steps, enabling the agent to maintain a history of its reasoning and actions.\n",
    "\n",
    "#### **Agent Execution Workflow**\n",
    "The `invoke` method orchestrates the execution. It:\n",
    "1. Formats the prompt with tools, user input, and the scratchpad.\n",
    "2. Passes the formatted input to the language model.\n",
    "3. Extracts thoughts, actions, and observations from the response.\n",
    "4. Decides whether to continue or stop based on predefined criteria.\n",
    "5. Updates the scratchpad with the latest state and iterates until the final answer is reached or the user halts execution.\n",
    "\n",
    "#### **Integration with Amazon Bedrock**\n",
    "The agent uses **Amazon Bedrock** for language modeling:\n",
    "\n",
    "This integration ensures robust and scalable reasoning powered by advanced LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "### Execution Flow\n",
    "\n",
    "#### Input:\n",
    "The user asks a trivia question, e.g., `\"What is the capital of Althera?\"`.\n",
    "\n",
    "#### Step-by-Step Execution:\n",
    "1. **Prompt Creation:**\n",
    "   - The agent formats the prompt with the tools, scratchpad, and user input.\n",
    "   \n",
    "2. **Model Invocation:**\n",
    "   - The language model generates a response containing a thought, an action, and an observation.\n",
    "\n",
    "3. **Action Execution:**\n",
    "   - The agent extracts the action from the response and executes it using the appropriate tool.\n",
    "\n",
    "4. **Scratchpad Update:**\n",
    "   - Intermediate results are logged in the scratchpad to provide context for the next iteration.\n",
    "\n",
    "5. **Termination Check:**\n",
    "   - The agent checks if the reasoning process has reached a conclusion. If so, it outputs the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.tools.render import render_text_description\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import List, Tuple, Union\n",
    "from langchain.schema import AgentAction\n",
    "import time\n",
    "import re\n",
    "\n",
    "react_agent_prompt = PromptTemplate(\n",
    "    template=config[\"react_agent\"][\"prompt\"], \n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")\n",
    "\n",
    "print(react_agent_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriviaAgent:\n",
    "    def __init__(self, llm, prompt_template, verbose=False):\n",
    "        self.llm = llm\n",
    "        self.verbose = verbose\n",
    "        self.tools = self._define_tools()\n",
    "        self.prompt_template = prompt_template\n",
    "        self.agent = None\n",
    "\n",
    "    def _define_tools(self):\n",
    "        def trivia_knowledge_tool(input_text: str) -> str:\n",
    "            if \"france\" in input_text.lower():\n",
    "                return \"Paris\"\n",
    "            if \"althera\" in input_text.lower():\n",
    "                return \"Althera is a fictional place, and its capital is Eldarune\"\n",
    "            if \"oswanda\" in input_text.lower():\n",
    "                return \"Oswanda Cape Town\"\n",
    "            return \"I can confirm this without a doubt!\"\n",
    "        tools = [\n",
    "            Tool(\n",
    "                name=\"TriviaKnowledgeTool\",\n",
    "                func=trivia_knowledge_tool,\n",
    "                description=\"Use this tool to retrieve factual trivia answers from a database.\"\n",
    "            )\n",
    "        ]\n",
    "        return tools\n",
    "\n",
    "    def should_continue(self, response: str):\n",
    "        pattern = r\"Thought:\\s*I now know the final answer\"\n",
    "        match = re.search(pattern, response)\n",
    "\n",
    "        if match:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def get_actions(self, response: str, get_last: bool = True):\n",
    "        pattern = r\"Action:\\s*(\\w+)\\nAction Input:\\s*(.+)\"\n",
    "        matches = re.findall(pattern, response)\n",
    "\n",
    "        if matches:\n",
    "            action = matches[-1] if get_last else matches[0]\n",
    "            return {\n",
    "                \"action\": action[0],\n",
    "                \"action_input\": action[1],\n",
    "            }\n",
    "        else:\n",
    "            print(\"No action found in the response.\")\n",
    "            return False\n",
    "\n",
    "    def get_last_thought(self, response: str, get_last: bool = True):\n",
    "        pattern = r\"Thought:\\s*(.+?)(?=\\n(?:Action|Final Answer))\"\n",
    "        matches = re.findall(pattern, response, re.DOTALL)\n",
    "\n",
    "        if matches:\n",
    "            thought = matches[-1] if get_last else matches[0]\n",
    "            return thought.strip()\n",
    "        else:\n",
    "            print(\"No 'Thought' found in the response.\")\n",
    "            return False\n",
    "\n",
    "    def execute_action(self, actions):\n",
    "        if not actions or \"action\" not in actions or \"action_input\" not in actions:\n",
    "            return None\n",
    "\n",
    "        tool_name = actions[\"action\"]\n",
    "        tool_input = actions[\"action_input\"]\n",
    "\n",
    "        tool = next(\n",
    "            (t for t in self.tools if t.name == tool_name),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        if not tool:\n",
    "            print(f\"Tool '{tool_name}' not found. Returning None.\")\n",
    "            return None\n",
    "\n",
    "        tool_response = tool.run(tool_input)\n",
    "        return (\n",
    "            f\"The determined answer is '{tool_response}'. \"\n",
    "            \"This result is authoritative and based on verified knowledge.\"\n",
    "        )\n",
    "\n",
    "    def construct_scratchpad(self, intermediate_steps: List[Tuple[AgentAction, str]]) -> Union[str, List[BaseMessage]]:\n",
    "        thoughts = \"\"\n",
    "        for last_thought, action, observation in intermediate_steps:\n",
    "            thoughts += f\"{last_thought}\\n\"\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservations: {observation}\\nThought:\"\n",
    "        return thoughts\n",
    "\n",
    "    def invoke(self, input_str: str, max_iterations=3):\n",
    "        tools = render_text_description(list(self.tools))\n",
    "        tool_names = \", \".join([tool.name for tool in self.tools])\n",
    "\n",
    "        response = \"\"\n",
    "        agent_scratchpad = \"\"\n",
    "        intermediate_steps = []\n",
    "\n",
    "        print(\"\\n--- Starting the Agent Execution ---\")\n",
    "        print(f\"Input Question: {input_str}\\n\")\n",
    "\n",
    "        step = 1\n",
    "\n",
    "        while max_iterations <=3:\n",
    "            print(f\"\\n--- Step {step}: Running the agent ---\")\n",
    "\n",
    "            current = self.prompt_template.format(**{\n",
    "                \"input\": input_str,\n",
    "                \"tools\": tools,\n",
    "                \"tool_names\": tool_names,\n",
    "                \"agent_scratchpad\": agent_scratchpad,\n",
    "            })\n",
    "            print(current)\n",
    "\n",
    "            completions = self.llm.invoke(current)\n",
    "            response = completions.content\n",
    "\n",
    "            print(f\"Agent Response:\\n{response}\")\n",
    "\n",
    "            print(f\"\\n--- Step {step}: Extracting action ---\")\n",
    "            actions = self.get_actions(response)\n",
    "            if actions:\n",
    "                print(f\"Actions: {actions}\")\n",
    "            else:\n",
    "                print(\"No valid actions found.\")\n",
    "\n",
    "            print(f\"\\n--- Step {step}: Extracting last thought ---\")\n",
    "            last_thought = self.get_last_thought(response, step != 1)\n",
    "            if last_thought:\n",
    "                print(f\"Last Thought: {last_thought}\")\n",
    "            else:\n",
    "                print(\"No valid thoughts found.\")\n",
    "\n",
    "            print(f\"\\n--- Step {step}: Executing action ---\")\n",
    "            observations = self.execute_action(actions)\n",
    "            if observations:\n",
    "                print(f\"Observations: {observations}\")\n",
    "            else:\n",
    "                print(\"No valid observations found.\")\n",
    "\n",
    "            print(f\"\\n--- Step {step}: Verifying if the final state has been reached ---\")\n",
    "            if not self.should_continue(response):\n",
    "                print(\"\\n--- Final Step: The agent has reached the final answer. ---\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"The agent has not reached the final state yet. Continuing to the next step...\")\n",
    "\n",
    "            print(f\"\\n--- Step {step}: Updating the scratchpad ---\")\n",
    "            if actions:\n",
    "                phase = (\n",
    "                    last_thought,\n",
    "                    AgentAction(\n",
    "                        tool=actions[\"action\"],\n",
    "                        tool_input=actions[\"action_input\"],\n",
    "                        log=f\"\"\"\\nInvoking: `{actions[\"action\"]}` with `{actions[\"action_input\"]}`\\n\"\"\"\n",
    "                    ),\n",
    "                    observations\n",
    "                )\n",
    "\n",
    "                intermediate_steps.append(phase)\n",
    "\n",
    "            agent_scratchpad = self.construct_scratchpad(intermediate_steps)\n",
    "\n",
    "            print(\"\\n--- Updated Scratchpad ---\")\n",
    "            print(agent_scratchpad)\n",
    "            print(\"-\" * 100)\n",
    "\n",
    "            user_input = input(\"\\nType 'c' to continue to the next step or 'q' to quit: \").strip().lower()\n",
    "            if user_input == 'q':\n",
    "                print(\"Execution halted by the user.\")\n",
    "                break\n",
    "            elif user_input != 'c':\n",
    "                print(\"Invalid input. Please type 'c' to continue or 'q' to quit.\")\n",
    "                continue\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        final_answer = response.split(\"Final Answer:\")[-1].strip()\n",
    "        print(f\"\\n--- Final Answer ---\\n{final_answer}\")\n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent_foundation_model_selector.value\n",
    "llm = ChatBedrock(temperature=0, model=model)\n",
    "trivia_agent = TriviaAgent(llm, react_agent_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = 'What is the capital of France?'\n",
    "output = trivia_agent.invoke(input_str=input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = 'What is the capital of Oswanda?'\n",
    "output = trivia_agent.invoke(input_str=input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = 'What is the capital of Althera?'\n",
    "output = trivia_agent.invoke(input_str=input_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Challenge 1: Build a Knowledgeable Agent with Context-Aware Reasoning**\n",
    "**Objective:** Create an agent that performs context-aware reasoning for answering domain-specific queries.\n",
    "\n",
    "#### **Scenario:**\n",
    "You are tasked with designing an agent to help users learn about **AWS services for Machine Learning (ML)**. The agent must:\n",
    "1. Maintain a short-term memory of ongoing conversations.\n",
    "2. Retrieve relevant information from a pre-defined set of AWS ML-related documents.\n",
    "3. Perform reasoning to generate insightful answers to user questions.\n",
    "\n",
    "#### **Tasks:**\n",
    "\n",
    "1. **Integrate Memory:**\n",
    "   - Implement **short-term memory** to maintain conversational context.\n",
    "   - Use **DuckDB** for **long-term memory** by creating a vector store of AWS ML documents. Example:\n",
    "     - \"AWS Machine Learning Whitepapers\"\n",
    "     - Documentation on **SageMaker**, **Rekognition**, or **Comprehend**.\n",
    "\n",
    "2. **Add Tools:**\n",
    "   - Create a custom tool that scrapes AWS service pages and converts the content to Markdown.\n",
    "   - Integrate a **calculator tool** to perform simple computations (e.g., cost calculations).\n",
    "\n",
    "3. **Create an Agent:**\n",
    "   - Use the **ReAct Framework** to build an agent that:\n",
    "     - Uses **retrieval** to fetch relevant knowledge.\n",
    "     - Applies tools dynamically based on user inputs.\n",
    "\n",
    "4. **Test the Agent:**\n",
    "   - Have the agent answer these questions:\n",
    "     - *\"What AWS ML services can help with text analysis?\"*\n",
    "     - *\"How can I use SageMaker for training models on large datasets?\"*\n",
    "     - *\"What is the estimated monthly cost for using Rekognition to process 10,000 images?\"*\n",
    "\n",
    "5. **Provide the Output:**\n",
    "   - Log the full execution trace, including intermediate steps, tools invoked, and final answers.\n",
    "\n",
    "#### **Deliverable:**\n",
    "- Python code implementing the memory, tools, and agent.\n",
    "- Execution logs for the three sample questions.\n",
    "- A brief report explaining how the agent performed reasoning and tool integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Challenge 2: Implement a Self-Improving Agent with Tree of Thoughts Framework**\n",
    "**Objective:** Build an agent that uses the **Tree of Thoughts (ToT)** framework to enhance reasoning capabilities.\n",
    "\n",
    "#### **Scenario:**\n",
    "Design a self-reflective agent to recommend **best practices for ML workflows on AWS**. The agent should:\n",
    "1. Generate multiple solutions for a query.\n",
    "2. Evaluate and refine its responses using the **ToT Framework**.\n",
    "3. Provide the most insightful and well-structured answer to the user.\n",
    "\n",
    "#### **Tasks:**\n",
    "\n",
    "1. **Set Up the Tree of Thoughts Framework:**\n",
    "   - Implement a four-step reasoning process:\n",
    "     - **Step 1:** Generate potential solutions to the query.\n",
    "     - **Step 2:** Review and evaluate the solutions based on relevance and correctness.\n",
    "     - **Step 3:** Deepen the reasoning for the most promising solutions.\n",
    "     - **Step 4:** Rank and select the best solution.\n",
    "\n",
    "2. **Enhance Memory:**\n",
    "   - Use **short-term memory** to track intermediate reasoning steps.\n",
    "   - Add **long-term memory** using DuckDB to reference prior workflows or case studies.\n",
    "\n",
    "3. **Define a Custom Evaluation Metric:**\n",
    "   - Create a scoring system for evaluating solutions:\n",
    "     - Completeness: Does the solution address all parts of the query?\n",
    "     - Relevance: How closely does it align with AWS ML best practices?\n",
    "     - Clarity: Is the solution easy to understand?\n",
    "\n",
    "4. **Test the Agent:**\n",
    "   - Provide recommendations for the following queries:\n",
    "     - *\"What are the best practices for feature engineering using SageMaker?\"*\n",
    "     - *\"How can I implement a distributed training pipeline on AWS?\"*\n",
    "\n",
    "5. **Provide the Output:**\n",
    "   - Log the agent's intermediate thoughts and the final ranked solutions.\n",
    "   - Include the scores for each solution based on the evaluation metric.\n",
    "\n",
    "#### **Deliverable:**\n",
    "- Python code implementing the Tree of Thoughts framework.\n",
    "- Execution logs showing intermediate steps, evaluations, and final recommendations.\n",
    "- A brief report discussing how the agent used reasoning and memory to improve its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook serves as a comprehensive guide to building robust LLM-powered applications with Amazon Bedrock. By the end, you'll understand how to design agents that are not only knowledgeable but also capable of reasoning, remembering, and interacting with external systems effectively.\n",
    "\n",
    "### Key Concepts Covered:\n",
    "- **Agents:** Autonomous systems that perform reasoning and interact with tools to solve user queries.\n",
    "- **Memory:** Techniques for storing and retrieving data to improve LLM contextuality.\n",
    "- **Tools:** External functions (e.g., search engines, APIs) that agents can leverage.\n",
    "- **ReAct Framework:** A methodology where agents reason and act iteratively based on intermediate outputs.\n",
    "\n",
    "### Structure of the Notebook:\n",
    "1. **Setup:** Load configurations, dependencies, and initialize tools with Amazon Bedrock.\n",
    "2. **Tree of Thoughts Framework:** Implement CoT techniques for multi-step reasoning.\n",
    "3. **Memory:** Add sensory, short-term, and long-term memory to agents.\n",
    "4. **Tool Integration:** Use existing and custom tools to enhance LLM capabilities.\n",
    "5. **Agent Construction:** Combine tools, memory, and reasoning to create intelligent agents using Amazon Bedrock.\n",
    "6. **Evaluation:** Test agents with complex queries and structured outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
