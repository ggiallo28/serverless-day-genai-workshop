{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73c6cd9",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Amazon Bedrock\n",
    "\n",
    "> *This notebook explores prompt engineering techniques for working with foundation models available in Amazon Bedrock.*\n",
    "\n",
    "Prompt engineering is a key skill in optimizing interactions with large language models. By crafting effective prompts, you can guide models to produce more accurate, relevant, and reliable outputs for your specific use cases.\n",
    "\n",
    "### Objectives\n",
    "- Understand the basics of prompt engineering.\n",
    "- Learn techniques to create better prompts for tasks such as summarization, question answering, and entity extraction.\n",
    "- Experiment with Amazon Bedrock models like Titan and Claude.\n",
    "\n",
    "### Pre-requisites\n",
    "- Access to Amazon Bedrock.\n",
    "- Completion of the Bedrock boto3 setup notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea5403",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's start by importing the required libraries and configuring Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import botocore.exceptions\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "aws_region = \"us-east-1\"\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "bedrock_management_client = boto3.client('bedrock', region_name=aws_region)\n",
    "bedrock_agent_client = boto3.client('bedrock-agent', region_name=aws_region)\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=aws_region)\n",
    "cloudformation_client = boto3.client('cloudformation', region_name=aws_region)\n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de6212-f249-4960-904f-f803b876e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Amazon Bedrock is a new service that makes foundation models accessible via an API. \n",
    "It provides a scalable, reliable, and secure way to build generative AI-based applications \n",
    "without managing any infrastructure. With Bedrock, you can quickly get started with foundation \n",
    "models from various providers, customize them with your data, and integrate them into your applications.\n",
    "This eliminates the complexities of building and maintaining machine learning models, allowing \n",
    "businesses to focus on innovation and accelerating time-to-market for their products. \n",
    "\n",
    "Foundation models offered through Bedrock are pre-trained on diverse datasets, making them \n",
    "highly capable for various generative AI tasks such as text generation, summarization, \n",
    "translation, and content creation. Users can access models from providers like Anthropic, AI21 Labs, \n",
    "and Amazonâ€™s own Titan models, giving them a wide range of options tailored to different use cases.\n",
    "\n",
    "Bedrock also supports easy customization of foundation models using your proprietary data, \n",
    "without requiring you to train models from scratch. This feature is particularly valuable for \n",
    "organizations that need models to align closely with their domain-specific knowledge and operational needs.\n",
    "\n",
    "Additionally, Bedrock provides seamless integration into existing workflows and applications, \n",
    "with built-in security and compliance features to ensure data privacy and governance. \n",
    "Organizations can confidently deploy generative AI capabilities while adhering to industry standards \n",
    "and regulations. With Amazon Bedrock, businesses of all sizes can unlock the power of generative AI \n",
    "to create innovative solutions and enhance productivity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648124b",
   "metadata": {},
   "source": [
    "## Example 1: Summarization\n",
    "\n",
    "In this example, we'll explore how prompt design affects the summarization task using the Titan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645eedb4-3ecd-4d43-9174-42a3ba717357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(prompt, parameters, model_id):\n",
    "    try:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"inputText\": prompt, \n",
    "                \"textGenerationConfig\": parameters\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        summary = response_body.get('results')[0].get('outputText')\n",
    "        print(\"Summary:\", summary)\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(\"Error invoking the model:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f172ef3-8bb5-4637-8534-7ad736fce542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-tg1-large',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b74fc3-940e-4d0a-b1d8-38eb77a17b01",
   "metadata": {},
   "source": [
    "### Experiment with Variations\n",
    "\n",
    "- Adjust the `temperature` parameter to control the randomness of the output.\n",
    "- Modify the instruction in the prompt for more detailed or concise summaries.\n",
    "- Use the `get_summary` function to complete this task.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Please summarize the following text:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"maxTokenCount\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "    \"topP\": 1.0\n",
    "}\n",
    "\n",
    "model_id = agent_foundation_model_selector.value\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8972dbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caab71-7d99-4f31-bcea-909201e33b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c8e8f89",
   "metadata": {},
   "source": [
    "## Example 2: Question Answering\n",
    "\n",
    "Here, we'll design prompts to extract specific answers from the model using Titan and Claude models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25403b47-5481-461a-8a82-46395b69f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_answer(qa_prompt, parameters, model_id):\n",
    "    try:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"inputText\": qa_prompt,\n",
    "                \"textGenerationConfig\": parameters\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        results = response_body.get('results')\n",
    "\n",
    "        if results and len(results) > 0:\n",
    "            answer = results[0].get('outputText', \"No answer generated.\")\n",
    "        else:\n",
    "            answer = \"No results found in the model response.\"\n",
    "\n",
    "        print(\"Answer:\", answer)\n",
    "        return answer\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(\"Error invoking the model:\", error)\n",
    "        return f\"Error: {error.response['Error']['Message']}\" if 'Error' in error.response else \"An unexpected error occurred.\"\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n",
    "        return \"An unexpected error occurred. Please check the input and try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b3359-cdcf-4bca-9e5d-8ae4df660357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-text-premier-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169f3ef-df21-4986-8fc8-746a6f3d1ab4",
   "metadata": {},
   "source": [
    "### Experiment with Context\n",
    "- Add more specific details to the context to observe how the model's answer changes.\n",
    "- Test the model with ambiguous or incomplete contexts.\n",
    "- Use the `get_model_answer` function to complete this task.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "question = \"What is Amazon Bedrock?\"\n",
    "\n",
    "qa_prompt = f\"\"\"\n",
    "Answer the question based on the context provided below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "model_id = agent_foundation_model_selector.value\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a610e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173916c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5d63f1",
   "metadata": {},
   "source": [
    "## Example 3: Entity Extraction\n",
    "In this task, we'll guide the model to extract structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72745740-3cfa-4149-b566-b18a87b8aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_text = \"\"\"\n",
    "Dear Customer Support Team,\n",
    "\n",
    "I hope this message finds you well. My name is John Doe, and I am reaching out to inquire about a couple of books that I am interested in purchasing from your store. The first book is titled 'Generative AI: A Comprehensive Guide', which I believe is one of your bestsellers. Could you confirm if this book is currently available in stock? If so, could you also let me know the price and estimated shipping times to New York City?\n",
    "\n",
    "Additionally, I am also interested in another book, 'Machine Learning for Beginners', which I couldnâ€™t find on your website. Could you check if this book is available and provide the same details as above?\n",
    "\n",
    "Finally, I would appreciate it if you could inform me about any ongoing discounts or promotions that might apply to these books or other related items.\n",
    "\n",
    "Looking forward to your response.\n",
    "\n",
    "Best regards,\n",
    "John Doe\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66de497-9cfa-49f9-8666-4de7664e4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(entity_prompt, parameters, model_id):\n",
    "    try:\n",
    "        body = json.dumps(\n",
    "            {\n",
    "                \"inputText\": entity_prompt,\n",
    "                \"textGenerationConfig\": parameters\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=model_id,\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        results = response_body.get('results')\n",
    "\n",
    "        if results and len(results) > 0:\n",
    "            entities = results[0].get('outputText', \"No entities extracted.\")\n",
    "        else:\n",
    "            entities = \"No results found in the model response.\"\n",
    "\n",
    "        print(\"Extracted Entities:\", entities)\n",
    "        return entities\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        print(\"Error invoking the model:\", error)\n",
    "        return f\"Error: {error.response['Error']['Message']}\" if 'Error' in error.response else \"An unexpected error occurred.\"\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n",
    "        return \"An unexpected error occurred. Please check the input and try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3441237-7414-47d0-86d1-77e7ce8884e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-text-premier-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e49f6",
   "metadata": {},
   "source": [
    "### Customizing Entity Extraction\n",
    "- Experiment with different entity types to extract.\n",
    "- Use tags or structured formats to make extraction easier.\n",
    "- Use the `extract_entities` function to complete this task.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "entity_prompt = f\"\"\"\n",
    "Extract the following information from the email:\n",
    "1. Name of the sender.\n",
    "2. Book title mentioned.\n",
    "3. Questions asked.\n",
    "\n",
    "Email:\n",
    "{email_text}\n",
    "\n",
    "Provide the answers in JSON format with keys: 'name', 'book', 'questions'.\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"maxTokenCount\": 200,\n",
    "    \"temperature\": 0.7,\n",
    "    \"topP\": 1.0\n",
    "}\n",
    "\n",
    "model_id = agent_foundation_model_selector.value\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb7fccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe1004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0568c5c9-df73-4205-8a58-0f2f26b61d27",
   "metadata": {},
   "source": [
    "## Prompt Engineering with LangChain AWS\n",
    "\n",
    "Building on the foundational knowledge of prompt engineering in Amazon Bedrock, this section explores the use of **LangChain AWS**, a high-level framework for seamless integration with Amazon Bedrock. With LangChain AWS, you can simplify interactions with foundation models, enabling rapid experimentation and optimization of generative AI applications.\n",
    "\n",
    "### Objectives\n",
    "- Extend prompt engineering techniques to **LangChain AWS** for enhanced flexibility.\n",
    "- Apply advanced techniques to tasks like summarization, question answering, and entity extraction.\n",
    "- Experiment with a structured framework to streamline foundation model interactions.\n",
    "\n",
    "### Why LangChain AWS?\n",
    "While Amazon Bedrock APIs provide direct access to foundation models, **LangChain AWS** abstracts low-level operations, enabling:\n",
    "- **Simplified workflows:** Manage prompts and responses efficiently.\n",
    "- **Custom chaining:** Combine tasks like summarization and entity extraction in a single flow.\n",
    "- **Model-agnostic flexibility:** Switch between foundation models without major code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca37154-0f62-40b6-b280-62a5ace9a0fc",
   "metadata": {},
   "source": [
    "### Explanation of Labels in Chat Prompts\n",
    "\n",
    "In the `ChatPromptTemplate` used within LangChain, labels such as `system`, `human`, and `assistant` serve distinct roles to structure conversations. Here's a quick explanation:\n",
    "\n",
    "- **`system`**: Represents the role of the AI assistant and provides high-level guidance on its behavior or objective. For example, it can set the context by specifying that the assistant is a summarizer or a general knowledge expert.\n",
    "  \n",
    "- **`human`**: Represents input from the user. This is where the specific prompt or question is provided, often using placeholders (like `{context}`) to insert dynamic content at runtime.\n",
    "\n",
    "- **`assistant`**: Represents responses generated by the AI model. This is typically used to pre-fill examples or specify expected behavior, though itâ€™s optional in many cases.\n",
    "\n",
    "These labels ensure clarity in conversations and guide the AI to respond appropriately within the defined context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40993e-bc9e-4002-a673-cb855d8eab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value='anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da8365-b4d2-498b-a65c-b02caba1f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "import json\n",
    "\n",
    "model_id = agent_foundation_model_selector.value\n",
    "llm = ChatBedrock(\n",
    "    model_id=model_id,\n",
    "    region=aws_region,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70c878-4d99-4901-8664-e3eb2b4cdefe",
   "metadata": {},
   "source": [
    "## Example 1: Summarization and Translation\n",
    "\n",
    "In this example, we'll demonstrate how to perform summarization and translation tasks using the LangChain AWS integration with Amazon Bedrock. By leveraging the flexibility of **`ChatPromptTemplate`**, we'll dynamically craft prompts for each task and invoke the Bedrock LLM for concise outputs. \n",
    "\n",
    "This approach allows for:\n",
    "- Efficient summarization of large text passages.\n",
    "- Dynamic translation between specified languages. \n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ea193-575d-4beb-af46-b406a9b82936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def get_summarization_chain(llm):\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that summarizes text concisely.\"),\n",
    "        (\"human\", \"Please summarize the following text:\\n{context}\")\n",
    "    ])\n",
    "\n",
    "    summarization_chain = summarization_prompt | llm\n",
    "\n",
    "    return summarization_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf358e3-360b-45cc-855b-da7924a79e36",
   "metadata": {},
   "source": [
    "### Using a Summarization Chain\n",
    "\n",
    "The code snippet below demonstrates how to use a summarization chain to process a given `context` and generate a summary. You can integrate it into your workflow to automate text summarization tasks.\n",
    "\n",
    "```python\n",
    "summarization_chain = [...]   # Initialize your translation chain\n",
    "inputs = {\"context\": \"<put-your-context-here>\"}\n",
    "response = summarization_chain.invoke(inputs)\n",
    "print(\"Summary:\", response.content)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130db1a-9cba-48f0-8f80-b749f6568064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1867a-895c-4c79-a846-a066cc321251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def ger_translation_chain(llm):\n",
    "    translation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    translation_chain = translation_prompt | llm\n",
    "\n",
    "    return translation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b12315-01b1-4949-94a0-842b46f2adbb",
   "metadata": {},
   "source": [
    "### Using a Translation Chain\n",
    "\n",
    "The code snippet below demonstrates how to use a translation chain to translate text from one language to another. You can integrate it into your workflow to automate translation tasks.\n",
    "\n",
    "```python\n",
    "translation_chain = [...]  # Initialize your translation chain\n",
    "translation_inputs = {\n",
    "    \"input_language\": \"English\",\n",
    "    \"output_language\": \"German\",\n",
    "    \"input\": \"I love programming.\"\n",
    "}\n",
    "response = translation_chain.invoke(translation_inputs)\n",
    "\n",
    "print(\"Translation:\", response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79b5ba-a530-4f24-a883-91eac7cbb42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1be285da-ec84-48e3-a5d7-1e0c63a2b6ad",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "- Modify the temperature to observe changes in output style.\n",
    "- Adjust the max tokens to control the length of the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0627a-1720-4d95-8f58-963a99a21af2",
   "metadata": {},
   "source": [
    "## Example 2: Question Answering\n",
    "We'll craft a prompt to retrieve specific information from a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68caa150-8b8d-447e-8d0e-22a4b2fc59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "def get_question_answering(llm):\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a knowledgeable assistant answering questions.\"),\n",
    "        (\"human\",\n",
    "    \"\"\"\n",
    "    Answer the question based on the context provided below:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    qa_chain = qa_prompt | llm\n",
    "\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997cce-77b9-4ede-9cd5-4048e62541a8",
   "metadata": {},
   "source": [
    "### Using a QA Chain\n",
    "\n",
    "The code snippet below demonstrates how to use a question-answering (QA) chain to extract answers from a given context. You can integrate it into your workflow to automate answering questions based on specific text inputs.\n",
    "\n",
    "```python\n",
    "qa_chain = [...]  # Initialize your QA chain\n",
    "question = \"What is Amazon Bedrock?\"\n",
    "qa_inputs = {\n",
    "    \"context\": \"<put-your-context-here>\",\n",
    "    \"question\": question\n",
    "}\n",
    "response = qa_chain.invoke(qa_inputs)\n",
    "\n",
    "print(\"Answer:\", response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331c126-d0b0-4d8c-b0a1-010092200b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39cd3e0b-0c8a-4224-88a4-f2a7e120c956",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "- Provide ambiguous or incomplete contexts to test the model's behavior.\n",
    "- Add specific details to the context to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0087c7a-d864-417e-95b8-8b7161a90abc",
   "metadata": {},
   "source": [
    "## Example 3: Entity Extraction\n",
    "Let's guide the model to extract structured information from an unstructured email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec0b48-0421-4e04-b067-14856a031ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def get_entity_extraction_chain(llm):\n",
    "\n",
    "    entity_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\", \n",
    "            \"You are a helpful assistant extracting structured information from emails.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"\"\"\n",
    "            Extract the following information from the email:\n",
    "            - Sender's name.\n",
    "            - Book title mentioned.\n",
    "            - Questions asked.\n",
    "\n",
    "            Email:\n",
    "            {email_text}\n",
    "\n",
    "            Provide the answers in JSON format with keys: 'name', 'book', 'questions'.\n",
    "            \"\"\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    entity_extraction_chain = entity_extraction_prompt | llm\n",
    "\n",
    "    return entity_extraction_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf22ec-5713-4023-9cfc-5c19a68b6a03",
   "metadata": {},
   "source": [
    "### Using an Entity Extraction Chain\n",
    "\n",
    "The code snippet below demonstrates how to use an entity extraction chain to identify entities from a given text. You can integrate it into your workflow to automate entity extraction tasks.\n",
    "\n",
    "```python\n",
    "entity_extraction_chain = [...]  # Initialize your NER chain\n",
    "entity_extraction_inputs = {\n",
    "    \"email_text\": \"<put-your-email-text-here>\"\n",
    "}\n",
    "\n",
    "response = entity_extraction_chain.invoke(entity_extraction_inputs)\n",
    "\n",
    "print(\"Extracted Entities:\", response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99bd98-7219-4dc5-8bfc-aecffbbec0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b34c07c-b2a7-4cbe-9024-25254eb97c61",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "- Try with different email structures and observe the extracted entities.\n",
    "- Test for emails with missing or incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1eada2-ed85-4fba-a695-855a5a7df760",
   "metadata": {},
   "source": [
    "## Example 4: Combined Chain\n",
    "In this example, we'll demonstrate how to combine multiple chains to perform summarization, translation, and information extraction sequentially. This approach allows for streamlined processing of text through different tasks, such as summarizing a given context, translating it into another language, and finally extracting structured information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fa4969-4735-4a59-8044-2ea25561acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a knowledgeable assistant answering questions.\"),\n",
    "    (\"human\", \"Answer the question based on the context provided below: Context: {context} Question: {question}\")\n",
    "])\n",
    "\n",
    "entity_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant extracting structured information. Provide the answers in JSON format with keys: 'key_points', 'models_mentioned', 'benefits'.\"),\n",
    "    (\"human\", \"Extract key points, models mentioned, and benefits from the following context: {context}\")\n",
    "])\n",
    "\n",
    "summarization_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that summarizes text concisely.\"),\n",
    "    (\"human\", \"Summarize the following information:\\nAnswer: {answer}\\nEntities: {entities}\")\n",
    "])\n",
    "\n",
    "qa_chain = {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")} | qa_prompt | llm | StrOutputParser()\n",
    "entity_extraction_chain = {\"context\": itemgetter(\"context\")} | entity_extraction_prompt | llm | StrOutputParser()\n",
    "\n",
    "summarization_chain = {\n",
    "    \"answer\": qa_chain,\n",
    "    \"entities\": entity_extraction_chain\n",
    "} | summarization_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892df8e-a0c6-4804-902c-dd7cb3fede9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"context\": context,\n",
    "    \"question\": \"What is not Amazon Bedrock?\"\n",
    "}\n",
    "\n",
    "results = summarization_chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a33f40-633c-4a63-9494-d6271c0ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e602e1-3aca-41f8-b612-c2cd24f519c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_chain.invoke(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1f1ca-3f84-41cc-b7a1-f35891b33e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entity_extraction_chain.invoke(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c403d9-dc2d-4119-bedc-f14e81fbd6a5",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "- Use various contexts to observe how the combined chain performs for summarization, question answering, and entity extraction.\n",
    "- Test with incomplete or ambiguous contexts and evaluate the adaptability of the combined chain.\n",
    "- Experiment with different questions and examine how well the chain handles diverse queries.\n",
    "- Modify the structure of the context and assess how it impacts the quality of extracted entities and the overall results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bf18a-33f2-47a4-84e8-d25a1beec62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3718aa58-f276-4fee-aae6-117eb707bb20",
   "metadata": {},
   "source": [
    "### **Challenge 1: Generate Multi-Lingual FAQs**\n",
    "**Objective:** Use prompt engineering techniques to create an FAQ page in multiple languages based on the provided context.\n",
    "\n",
    "**Scenario:**\n",
    "You are tasked with creating an FAQ section for a website about **Amazon Bedrock**. The FAQ should include a question and a short, concise answer in **English, Spanish, and German**.\n",
    "\n",
    "**Tasks:**\n",
    "1. Write a prompt that instructs the model to:\n",
    "   - Generate a list of 5 FAQs based on the provided context about Amazon Bedrock.\n",
    "   - Provide answers to each question.\n",
    "   - Translate the FAQs and answers into Spanish and German.\n",
    "\n",
    "2. Use **LangChain AWS** or the Bedrock API directly to:\n",
    "   - Generate the FAQ content.\n",
    "   - Ensure the translations are accurate and contextually relevant.\n",
    "   \n",
    "3. Output the FAQ content in a structured JSON format, like:\n",
    "   ```json\n",
    "   [\n",
    "       {\n",
    "           \"question\": \"What is Amazon Bedrock?\",\n",
    "           \"answer\": \"Amazon Bedrock is a service that makes foundation models accessible via an API.\",\n",
    "           \"translations\": {\n",
    "               \"es\": \"Â¿QuÃ© es Amazon Bedrock?\",\n",
    "               \"de\": \"Was ist Amazon Bedrock?\"\n",
    "           }\n",
    "       },\n",
    "       ...\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "**Bonus:**\n",
    "- Include a summarization of the FAQ content in **French** at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a2a57-db1f-4e56-827b-068b03382cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b43aa9e-191a-4be1-8962-903fc166f253",
   "metadata": {},
   "source": [
    "### **Challenge 2: Extract and Summarize Key Benefits for a Report**\n",
    "**Objective:** Extract and summarize the key benefits of Amazon Bedrock from the given context to create a concise report.\n",
    "\n",
    "**Scenario:**\n",
    "You need to prepare a report highlighting the **key benefits** of Amazon Bedrock for stakeholders. The report should include:\n",
    "- A list of 3-5 bullet points summarizing the benefits.\n",
    "- Structured extraction of models mentioned in the context.\n",
    "- A short summary of the key benefits for an executive audience.\n",
    "\n",
    "**Tasks:**\n",
    "1. Write a prompt that:\n",
    "   - Extracts the **key benefits** of Amazon Bedrock.\n",
    "   - Identifies and lists the foundation models mentioned in the context.\n",
    "   - Summarizes the benefits into a 2-3 sentence paragraph.\n",
    "\n",
    "2. Use **LangChain AWS** to:\n",
    "   - Extract and structure the benefits and models into a JSON format like:\n",
    "     ```json\n",
    "     {\n",
    "         \"benefits\": [\n",
    "             \"Easy integration into workflows\",\n",
    "             \"Wide range of foundation models\",\n",
    "             \"Customizable with proprietary data\"\n",
    "         ],\n",
    "         \"models\": [\n",
    "             \"Titan\",\n",
    "             \"Claude\",\n",
    "             \"Stable Diffusion\"\n",
    "         ],\n",
    "         \"summary\": \"Amazon Bedrock provides seamless integration of generative AI capabilities into workflows, offers diverse foundation models, and supports customization with proprietary data.\"\n",
    "     }\n",
    "     ```\n",
    "   \n",
    "3. Generate the content with clear formatting for inclusion in a report.\n",
    "\n",
    "**Bonus:**\n",
    "- Include a follow-up chain to translate the summary into **Japanese**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc980421-311f-442b-8784-314fd8e08a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71c8a66",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Prompt engineering, combined with the powerful capabilities of LangChain AWS and Amazon Bedrock models, allows you to enhance the effectiveness of foundation models for a wide range of generative AI tasks. By crafting clear prompts and experimenting with different configurations, you can tailor model responses to suit your specific use cases seamlessly.\n",
    "\n",
    "### Key Takeaways\n",
    "- Use clear and specific instructions in your prompts to guide the model effectively.\n",
    "- Experiment with temperature, token count, and top-p parameters to optimize performance.\n",
    "- Leverage LangChain's flexible API for seamless integration and streamlined interactions with foundation models.\n",
    "- Iterate on your prompts and configurations based on the model's output to refine and improve results.\n",
    "- Combine generative AI capabilities with LangChain AWS to unlock innovative solutions tailored to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fd9cf-2a94-4469-8f46-96c4dde3ca2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
