{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27786564",
   "metadata": {},
   "source": [
    "# Amazon Bedrock RAG Application with CloudFormation\n",
    "\n",
    "This notebook demonstrates how to build a Q&A application using Amazon Bedrock with Retrieval-Augmented Generation (RAG).\n",
    "Resources such as S3 buckets, IAM roles, and other infrastructure are provisioned using a CloudFormation template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187cb9f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's start by importing the required libraries and configuring Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31778d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import botocore.exceptions\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "import pprint as pp\n",
    "import time\n",
    "\n",
    "aws_region = \"us-east-1\"\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "\n",
    "def interactive_sleep(seconds):\n",
    "    \"\"\"Sleep interactively for the given number of seconds with progress.\"\"\"\n",
    "    for i in range(seconds):\n",
    "        print(f\"Sleeping... {i + 1}/{seconds} seconds\", end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=aws_region)\n",
    "bedrock_management_client = boto3.client('bedrock', region_name=aws_region)\n",
    "bedrock_agent_client = boto3.client('bedrock-agent', region_name=aws_region)\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime', region_name=aws_region)\n",
    "cloudformation_client = boto3.client('cloudformation', region_name=aws_region)\n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb722",
   "metadata": {},
   "source": [
    "### Load CloudFormation Template\n",
    "\n",
    "The CloudFormation template defines resources such as S3 buckets for data storage, IAM roles for access control, and Amazon Bedrock configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import yaml\n",
    "from pygments import highlight, lexers, formatters\n",
    "\n",
    "cloudformation_client = boto3.client('cloudformation')\n",
    "template_file_path = 'bedrock_rag_template.yaml'\n",
    "\n",
    "\n",
    "def get_template():\n",
    "    try:\n",
    "        with open(template_file_path, 'r') as template_file:\n",
    "            cloudformation_template = template_file.read()\n",
    "        print(\"CloudFormation template loaded successfully.\")\n",
    "        return cloudformation_template\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{template_file_path}' was not found.\")\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error parsing YAML file: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "cloudformation_template = get_template()\n",
    "colorful_yaml = highlight(\n",
    "    cloudformation_template,\n",
    "    lexers.YamlLexer(),\n",
    "    formatters.TerminalFormatter()\n",
    ")\n",
    "\n",
    "print(colorful_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336fbac0",
   "metadata": {},
   "source": [
    "### Deploy CloudFormation Stack\n",
    "\n",
    "Deploy the stack using the loaded template. This creates all required resources for the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efa985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "unique_id = str(uuid.uuid4())\n",
    "\n",
    "stack_name = f'BedrockRAGStack-{unique_id}'\n",
    "caller_identity = !aws sts get-caller-identity\n",
    "caller_identity = json.loads(''.join(caller_identity))\n",
    "\n",
    "try:\n",
    "    cloudformation_template = get_template()\n",
    "    response = cloudformation_client.create_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=cloudformation_template,\n",
    "        Capabilities=['CAPABILITY_NAMED_IAM'],\n",
    "        Parameters=[\n",
    "            {\n",
    "                'ParameterKey': 'CreateKnowledgeBase',\n",
    "                'ParameterValue': 'false'\n",
    "            },\n",
    "            {\n",
    "                'ParameterKey': 'UUID',\n",
    "                'ParameterValue': unique_id[:8]\n",
    "            },\n",
    "            {\n",
    "                'ParameterKey': 'CallerIdentity',\n",
    "                'ParameterValue': caller_identity[\"Arn\"]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Stack creation initiated. Stack ID:\", response['StackId'])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{template_file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1fea21",
   "metadata": {},
   "source": [
    "### Wait for Stack Completion\n",
    "\n",
    "Wait until the stack creation is complete before proceeding to use the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9229bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stack(stack_name):\n",
    "    \"\"\"\n",
    "    Delete the specified CloudFormation stack.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Deleting stack {stack_name}...\")\n",
    "        cloudformation_client.delete_stack(StackName=stack_name)\n",
    "        print(f\"Delete request sent for stack {stack_name}.\")\n",
    "        wait_for_stack(stack_name, 'DELETE_COMPLETE')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting the stack: {e}\")\n",
    "\n",
    "\n",
    "def wait_for_stack(stack_name, expected_status):\n",
    "    \"\"\"\n",
    "    Wait for the stack to reach an expected status.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            response = cloudformation_client.describe_stacks(StackName=stack_name)\n",
    "            stack_status = response['Stacks'][0]['StackStatus']\n",
    "            print(f\"Stack status: {stack_status}. Waiting...\")\n",
    "            if stack_status == expected_status:\n",
    "                print(f\"Stack {stack_name} reached expected status: {expected_status}.\")\n",
    "                break\n",
    "            elif stack_status in ['CREATE_FAILED', 'ROLLBACK_COMPLETE', 'DELETE_FAILED']:\n",
    "                raise Exception(f\"Stack operation failed with status: {stack_status}\")\n",
    "        except cloudformation_client.exceptions.ClientError as e:\n",
    "            if 'does not exist' in str(e) and expected_status == 'DELETE_COMPLETE':\n",
    "                print(f\"Stack {stack_name} deleted successfully.\")\n",
    "                break\n",
    "            elif 'does not exist' in str(e):\n",
    "                raise Exception(f\"Stack {stack_name} does not exist.\")\n",
    "            else:\n",
    "                raise\n",
    "        time.sleep(30)\n",
    "\n",
    "\n",
    "def wait_for_stack_or_delete(status='CREATE_COMPLETE'):\n",
    "    try:\n",
    "        wait_for_stack(stack_name, status)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        delete_stack(stack_name)\n",
    "\n",
    "\n",
    "wait_for_stack_or_delete(status='CREATE_COMPLETE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d772a",
   "metadata": {},
   "source": [
    "### Retrieve Stack Outputs\n",
    "\n",
    "Get the outputs from the stack, such as the S3 bucket name and IAM role ARN, to use them in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69180735",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cloudformation_client.describe_stacks(StackName=stack_name)\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "\n",
    "results = {}\n",
    "for output in outputs:\n",
    "    results[output['OutputKey']] = output['OutputValue']\n",
    "    print(f\"{output['OutputKey']}: {output['OutputValue']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd75f2-70de-43a7-8ce1-94fea0a07f2e",
   "metadata": {},
   "source": [
    "### Create Vector Index\n",
    "\n",
    "Create an empty vector index to store the embeddings of text data and wait for its stabilization before proceeding to the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb2536-2d81-49d1-8bdd-af9345a05f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, RequestError\n",
    "\n",
    "\n",
    "INDEX_DIMENSION = int(os.getenv(\"INDEX_DIMENSION\", 256))\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", aws_region)\n",
    "HOST = results[\"OpenSearchServerlessCollectionEndpoint\"].replace('https://', '')\n",
    "SERVICE = \"aoss\"\n",
    "INDEX_NAME = os.getenv(\"VECTOR_INDEX_NAME\", f\"kb-index-{unique_id[:8]}\")\n",
    "VECTOR_FIELD_NAME = os.getenv(\"VECTOR_FIELD_NAME\", \"vector\")\n",
    "TEXT_FIELD_NAME = os.getenv(\"TEXT_FIELD_NAME\", \"AMAZON_BEDROCK_TEXT_CHUNK\")\n",
    "METADATA_FIELD_NAME = os.getenv(\"METADATA_FIELD_NAME\", \"AMAZON_BEDROCK_METADATA\")\n",
    "TIMEOUT_SECONDS = 60\n",
    "\n",
    "INDEX_BODY = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": \"true\",\n",
    "        \"number_of_shards\": 1,\n",
    "        \"knn.algo_param.ef_search\": 512,\n",
    "        \"number_of_replicas\": 0,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            VECTOR_FIELD_NAME: {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": INDEX_DIMENSION,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                },\n",
    "            },\n",
    "            TEXT_FIELD_NAME: {\"type\": \"text\"},\n",
    "            METADATA_FIELD_NAME: {\"type\": \"text\"},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_opensearch_client(host: str, region: str, service: str) -> OpenSearch:\n",
    "    \"\"\"Build and return an OpenSearch client.\"\"\"\n",
    "    try:\n",
    "        print(f\"Initializing OpenSearch client for host: {host}, region: {region}\")\n",
    "        credentials = boto3.Session().get_credentials()\n",
    "        awsauth = AWSV4SignerAuth(credentials, region, service)\n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': host, 'port': 443}],\n",
    "            http_auth=awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            timeout=300,\n",
    "        )\n",
    "        print(\"OpenSearch client initialized successfully.\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenSearch client: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_index(client: OpenSearch, index_name: str, body: dict) -> None:\n",
    "    \"\"\"Create a vector index in OpenSearch Serverless.\"\"\"\n",
    "    try:\n",
    "        print(f\"Creating index: {index_name}\")\n",
    "        response = client.indices.create(index=index_name, body=body)\n",
    "        print(\"Index created successfully:\")\n",
    "        print(json.dumps(response, indent=2))\n",
    "        print(f\"Waiting {TIMEOUT_SECONDS} seconds for index stabilization...\")\n",
    "        time.sleep(TIMEOUT_SECONDS)\n",
    "    except RequestError as e:\n",
    "        if e.info.get(\"status\") == 400 and \"already exists\" in str(e.error):\n",
    "            print(f\"Index {index_name} already exists. Skipping creation.\")\n",
    "        else:\n",
    "            print(f\"RequestError while creating index: {str(e)}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while creating the index: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Starting OpenSearch index creation process...\")\n",
    "    client = get_opensearch_client(HOST, AWS_REGION, SERVICE)\n",
    "    create_index(client, INDEX_NAME, INDEX_BODY)\n",
    "    print(\"OpenSearch index creation completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during the process: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3af406-77ed-43bd-ba72-c5e91121b8b5",
   "metadata": {},
   "source": [
    "### Update Stack to create Bedrock's Knowledge Base\n",
    "\n",
    "Update the CloudFormation stack, after ensuring the vector index is stable, to create a knowledge base in Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9c1eb-cc62-4cdd-bace-eae648d99dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cloudformation_template = get_template()\n",
    "    response = cloudformation_client.update_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=cloudformation_template,\n",
    "        Capabilities=['CAPABILITY_NAMED_IAM'],\n",
    "        Parameters=[\n",
    "            {\n",
    "                'ParameterKey': 'CreateKnowledgeBase',\n",
    "                'ParameterValue': 'true'\n",
    "            },\n",
    "            {\n",
    "                'ParameterKey': 'UUID',\n",
    "                'ParameterValue': unique_id[:8]\n",
    "            },\n",
    "            {\n",
    "                'ParameterKey': 'CallerIdentity',\n",
    "                'ParameterValue': caller_identity[\"Arn\"]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Stack update initiated. Stack ID:\", response['StackId'])\n",
    "\n",
    "except cloudformation_client.exceptions.ClientError as e:\n",
    "    if 'No updates are to be performed' in str(e):\n",
    "        print(\"No changes detected. The stack is already up-to-date.\")\n",
    "    else:\n",
    "        print(f\"An error occurred during stack update: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{template_file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "wait_for_stack_or_delete(status='UPDATE_COMPLETE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691278ec-0102-4d84-837f-9d603e684e77",
   "metadata": {},
   "source": [
    "### Update Stack Outputs\n",
    "\n",
    "Get the updated outputs from the stack after creating the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900194a-29ee-4882-bb8f-8059161aca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r stack_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e35b1e-b694-4244-99e9-1af3da4cf3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cloudformation_client.describe_stacks(StackName=stack_name)\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "\n",
    "results = {}\n",
    "for output in outputs:\n",
    "    results[output['OutputKey']] = output['OutputValue']\n",
    "    print(f\"{output['OutputKey']}: {output['OutputValue']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ba6aa",
   "metadata": {},
   "source": [
    "### Ingest Data into Knowledge Base\n",
    "\n",
    "Load documents into the Knowledge Base using the S3 bucket provisioned by the CloudFormation stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670acd0-cc5c-4387-9108-9a1fbd6cef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def download_and_upload_to_s3(data_root, files_to_download, bucket_name):\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    print(\"Downloading shareholder letters...\")\n",
    "    for url, filename in files_to_download.items():\n",
    "        file_path = os.path.join(data_root, filename)\n",
    "        urlretrieve(url, file_path)\n",
    "        print(f\"Downloaded: {file_path}\")\n",
    "    print(\"\\nUploading files to S3...\")\n",
    "    for filename in files_to_download.values():\n",
    "        local_file_path = os.path.join(data_root, filename)\n",
    "        s3_key = f\"{data_root}/{filename}\"\n",
    "        s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "        print(f\"Uploaded: s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "    print(\"\\nAll files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b4a9a-8efb-4475-b98e-149e67b11903",
   "metadata": {},
   "source": [
    "Use the `download_and_upload_to_s3` Function to Manage Files in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./kb_financials/\"\n",
    "FILES_TO_DOWNLOAD = {\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf': 'AMZN-2022-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf': 'AMZN-2021-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf': 'AMZN-2020-Shareholder-Letter.pdf',\n",
    "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf': 'AMZN-2019-Shareholder-Letter.pdf',\n",
    "}\n",
    "\n",
    "bucket_name = results[\"S3BucketName\"]\n",
    "\n",
    "download_and_upload_to_s3(DATA_ROOT, FILES_TO_DOWNLOAD, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e989cfa-b71b-47d0-9136-c59417db05f3",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS.\n",
    "\n",
    "```python\n",
    "knowledge_base_id = results['BedrockKnowledgeBaseId']\n",
    "data_source_id = results[\"BedrockDataSourceId\"].split(\"|\")[1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b8ffa-3880-43d5-8868-604d4310f5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05b1e637-ccf2-4cf8-ad22-e358b7cd9622",
   "metadata": {},
   "source": [
    "Use the below example code to start the ingestion_job and show the result\n",
    "\n",
    "```python\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId = [...], \n",
    "    dataSourceId = [...]\n",
    ")\n",
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95806b08-31ac-4046-bd9e-85df0efe7932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eace1abd-cb8d-4aae-940d-d99ab4fe6331",
   "metadata": {},
   "source": [
    "Wait Until the Sync Job is Complete. Use the example code below to verify when the ingestion job is complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14536a-44d8-4208-991c-3ecda7a291ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion_job_id = job[\"ingestionJobId\"]\n",
    "\n",
    "while job['status'] != 'COMPLETE':\n",
    "    get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId=[...],\n",
    "        dataSourceId=[...],\n",
    "        ingestionJobId=[...]\n",
    "    )\n",
    "    job = get_job_response[\"ingestionJob\"]\n",
    "    interactive_sleep(30)\n",
    "\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7771a9-0706-4ad4-8a77-3b09b6305661",
   "metadata": {},
   "source": [
    "Print the knowledge base Id in bedrock, that corresponds to the Opensearch index in the collection we created before, we will use it for the invocation later\n",
    "\n",
    "```python\n",
    "knowledge_base_id = results['BedrockKnowledgeBaseId']\n",
    "pp.pprint(knowledge_base_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee8e30-889b-43a6-b631-d2a34770e975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddecc8-35df-461c-be07-29f2169542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store knowledge_base_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304a054",
   "metadata": {},
   "source": [
    "### Query Knowledge Base Using Amazon Bedrock\n",
    "\n",
    "Use Amazon Bedrock's Retrieve API to query the knowledge base and get relevant results.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for solutions</summary>\n",
    "    \n",
    "```python\n",
    "response = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery={\"text\": query},\n",
    "    knowledgeBaseId=knowledge_base_id,\n",
    "    retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": 5}}\n",
    ")\n",
    "\n",
    "retrieved_chunks = response['retrievalResults']\n",
    "for chunk in retrieved_chunks:\n",
    "    print(chunk['content']['text'])\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84a1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is Amazon's strategy for Generative AI?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a1d8d-f1a4-4afa-a125-e7efd57860fc",
   "metadata": {},
   "source": [
    "### Extract the text chunks from the retrieveAPI response\n",
    "\n",
    "In the cell below, we will fetch the context from the retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07030396-f985-44e5-a334-d9769340664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(retrieved_chunks):\n",
    "    contexts = []\n",
    "    for retrieved_chunk in retrieved_chunks: \n",
    "        contexts.append(retrieved_chunk['content']['text'])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05670a2-57b0-4da0-a0fd-45649eabe165",
   "metadata": {},
   "source": [
    "Use the `get_contexts` function to visualize the context.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "contexts = get_contexts(retrieved_chunks)\n",
    "pp.pprint(contexts)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70affc2c-76ba-4452-ab77-bf46d2a324dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfaf3de8-8117-4129-a894-876c0062b049",
   "metadata": {},
   "source": [
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075b95c-da2b-4095-9e3b-420b0906b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, knowledge_base_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': knowledge_base_id,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_model_arn(model_id):\n",
    "    return f'arn:aws:bedrock:{aws_region}::foundation-model/{model_id[-1]}'\n",
    "\n",
    "\n",
    "def visualize_citations(generated_text, citations):\n",
    "    contexts = []\n",
    "    for citation in citations:\n",
    "        retrievedReferences = citation[\"retrievedReferences\"]\n",
    "        for reference in retrievedReferences:\n",
    "            contexts.append(reference[\"content\"][\"text\"])\n",
    "    print(f\"---------- Generated using {model_id[0]}:\")\n",
    "    pp.pprint(generated_text )\n",
    "    print(f'---------- The citations for the response generated by {model_id[0]}:')\n",
    "    pp.pprint(contexts)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e1fc3-924b-442f-9fdf-4889ea1603e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_model_ids = [ \n",
    "    [\"Claude 3 Sonnet\", \"anthropic.claude-3-sonnet-20240229-v1:0\"], \n",
    "    [\"Claude 3 Haiku\", \"anthropic.claude-3-haiku-20240307-v1:0\"]\n",
    "]\n",
    "\n",
    "query = \"What is Amazon's doing in the field of generative AI?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636bfc9-c671-4f5a-81aa-0ffa4dc38100",
   "metadata": {},
   "source": [
    "Iterate over the `claude_model_ids`, retrieve the `model_arn` using the `get_model_arn` function, and apply the query for each model using the `ask_bedrock_llm_with_knowledge_base` function. Then, extract both the `generated_text` and the `citations`, and use the `visualize_citations` function to display the result.\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "for model_id in claude_model_ids:\n",
    "    model_arn = get_model_arn(model_id)\n",
    "    response = ask_bedrock_llm_with_knowledge_base(query, model_arn, knowledge_base_id)\n",
    "    generated_text = response['output']['text']\n",
    "    citations = response[\"citations\"]\n",
    "    visualize_citations(generated_text, citations)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a3c7e-e1c3-454c-90ea-dcfe1b949710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b20160-f2bc-49e7-8645-689f0b588680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c2668e0",
   "metadata": {},
   "source": [
    "### Generate Response Using Amazon Bedrock\n",
    "\n",
    "Use the retrieved context to generate a response with an Amazon Bedrock foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdab273-a68d-472e-9a97-1a9d8fa31b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Amazon\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', []) if \"Nova\" not in model['modelName'] \n",
    "    ],\n",
    "    value='amazon.titan-text-express-v1',\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = None  # Insert your query text here\n",
    "\n",
    "if query:\n",
    "    retrieved_chunks = bedrock_agent_runtime_client.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=knowledge_base_id,\n",
    "        retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": 5}}\n",
    "    )['retrievalResults']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Human: Use the following context to answer the question:\n",
    "\n",
    "    {retrieved_chunks}\n",
    "\n",
    "    Question: {query}\n",
    "    Assistant:\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps({\"inputText\": prompt}),\n",
    "        modelId=agent_foundation_model_selector.value,\n",
    "        accept='application/json',\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    print(query)\n",
    "    print(\"Generated Response:\", response_body['results'][0]['outputText'])\n",
    "else:\n",
    "    raise ValueError(\"No query provided. Please define the 'query' variable with a valid input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a3a6c-eeff-4a5b-899e-f5060d7bcc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af234398-9dad-46e3-8d56-0e041acd253b",
   "metadata": {},
   "source": [
    "### Prompt specific to the model to personalize responses \n",
    "\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. We will provide the `Retrieve API` responses from above as a part of the `{contexts}` in the prompt for the model to refer to, along with the user `query`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcbb4a5-1eff-4c1b-9ac6-305f0b82a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = None  # Insert your query text here\n",
    "\n",
    "if query:\n",
    "    retrieved_chunks = bedrock_agent_runtime_client.retrieve(\n",
    "        retrievalQuery={\"text\": query},\n",
    "        knowledgeBaseId=knowledge_base_id,\n",
    "        retrievalConfiguration={\"vectorSearchConfiguration\": {\"numberOfResults\": 5}}\n",
    "    )['retrievalResults']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "    Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    <context>\n",
    "    {retrieved_chunks}\n",
    "    </context>\n",
    "\n",
    "    <question>\n",
    "    {query}\n",
    "    </question>\n",
    "\n",
    "    The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "else:\n",
    "    raise ValueError(\"No query provided. Please define the 'query' variable with a valid input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104414b-5952-42c7-a22d-bdf6f6a0cabe",
   "metadata": {},
   "source": [
    "### Invoke Foundation Model from Amazon Bedrock\n",
    "\n",
    "Prepare the JSON payload to be used with the `mistral.mistral-7b` foundation model from Amazon Bedrock.\n",
    "\n",
    "This model is a 7B dense Transformer, fast-deployed, and easily customizable. It is small yet powerful for various use cases, including:\n",
    "\n",
    "- **Maximum tokens**: 8K\n",
    "- **Languages**: English\n",
    "- **Supported use cases**: Text summarization, structuration, question answering, and code completion\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "mistral_payload = json.dumps({\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9\n",
    "})\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7221d7-5940-4b31-86f4-d405983acf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4335a35-4343-4ba9-905a-6a4d4933bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Mistral AI\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eea7b-631c-44e4-93f1-cf74d879c433",
   "metadata": {},
   "source": [
    "Change `modelId` to use a different version from the model provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddaeac6-b60a-42b5-9d4c-fbb8517cc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "\n",
    "response = bedrock_runtime_client.invoke_model(\n",
    "    body=mistral_payload,\n",
    "    modelId=agent_foundation_model_selector.value,\n",
    "    accept=accept,\n",
    "    contentType=contentType\n",
    ")\n",
    "response_body = json.loads(response.get('body').read())\n",
    "response_text = response_body.get('outputs')[0]['text']\n",
    "\n",
    "print(query)\n",
    "print(response_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728765e-6cd0-485b-b70c-62b7c03a3166",
   "metadata": {},
   "source": [
    "### LangChain Integration for Q&A Applications\n",
    "\n",
    "Building on the previous sections, we now integrate LangChain to enhance the Q&A application using the Retrieve API provided by Knowledge Bases for Amazon Bedrock. LangChain allows seamless orchestration of retrieval-augmented generation workflows, enabling advanced applications to leverage the knowledge base efficiently.\n",
    "\n",
    "In this phase, we focus on querying the knowledge base for document chunks based on similarity search, utilizing LangChain's retriever integration. These retrieved results are then passed to the **Anthropic Claude V3.5** model, which processes the query and context to provide precise and context-aware answers.\n",
    "\n",
    "This integration demonstrates how LangChain simplifies the pipeline by managing retrieval, chunking, and contextual query enhancements, ensuring a streamlined workflow for leveraging Bedrock's powerful capabilities in Q&A scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1bcbf-a41a-46d1-abc9-e4738af39938",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_foundation_model_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        (model['modelName'], model['modelId']) \n",
    "        for model in bedrock_management_client.list_foundation_models(\n",
    "            byProvider=\"Anthropic\",\n",
    "            byOutputModality=\"TEXT\",\n",
    "            byInferenceType=\"ON_DEMAND\"\n",
    "        ).get('modelSummaries', [])\n",
    "    ],\n",
    "    value=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    description='FM:',\n",
    "    disabled=False,\n",
    ")\n",
    "agent_foundation_model_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d4687-637e-4518-ab01-80a36ee6276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=agent_foundation_model_selector.value,\n",
    "    client = bedrock_runtime_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39e15a-101b-4bd9-95b9-f4f12360f04c",
   "metadata": {},
   "source": [
    "Create a `AmazonKnowledgeBasesRetriever` object from LangChain which will call the `Retreive API` provided by Knowledge Bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workï¬‚ows on top of the semantic search results. The output of the `Retrieve API` includes the the `retrieved text chunks`, the `location type` and `URI` of the source data, as well as the relevance `scores` of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5ab82-acb7-4e22-ba67-cb8c34188894",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"By what percentage did AWS revenue grow year-over-year in 2022?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db6a94-debb-4539-be61-0d2499bdaec2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=knowledge_base_id,\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": 4,\n",
    "            \"overrideSearchType\": \"SEMANTIC\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594f1f6-9da5-4a13-96c1-c6fc6ca8bb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afa55b-38e1-470a-9913-6be40a438914",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(input=query)\n",
    "pp.pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb84f5c-a094-49cd-8579-9efb01c2d8ac",
   "metadata": {},
   "source": [
    "## Prompt specific to the model to personalize responses\n",
    "Here, we will use the specific prompt below for the model to act as a financial advisor AI system that will provide answers to questions by using fact based and statistical information when possible. We will provide the Retrieve API responses from above as a part of the `{context}` in the prompt for the model to refer to, along with the user `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfaf473-ac0b-49dc-89ad-ca2ffd8d1dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are a financial advisor AI system, and provides answers to questions by using fact based and statistical information when possible. \n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "claude_prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0e5e7-0feb-4fb8-bedd-6a40b2265ce1",
   "metadata": {},
   "source": [
    "### Building a Q&A Application Using RetrievalQA Chain\n",
    "\n",
    "Integrate the retriever and the LLM (defined earlier) with the `RetrievalQA` chain to create a Question & Answer (Q&A) application.\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "```\n",
    "\n",
    "For more information on the `RetrievalQA` chain, refer to the official documentation:\n",
    "\n",
    "- [RetrievalQA API Reference](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html)  \n",
    "- [Chain Types Documentation](https://python.langchain.com/v0.1/docs/modules/chains/)\n",
    "\n",
    "<details>\n",
    "<summary>Click here for the solution</summary>\n",
    "    \n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": claude_prompt}\n",
    ")\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c8bc9-d65d-4547-9bf1-98995f3f71eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e5c96-3b98-4b55-a1b4-3402924f122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = qa.invoke(query)\n",
    "pp.pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3fd8f-aba3-43d2-bcf3-3f6415ea499f",
   "metadata": {},
   "source": [
    "### Extracting and Formatting the Response with References\n",
    "\n",
    "The following code demonstrates how to extract the `response` from the `answer` variable and format it to include references in the specified syntax:\n",
    "\n",
    "#### Desired Output Syntax:\n",
    "\n",
    "The output should present the response followed by the references in the format:\n",
    "\n",
    "```\n",
    "Response:\n",
    "<extracted-response-text>\n",
    "\n",
    "References:\n",
    "1. <reference-1>\n",
    "2. <reference-2>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8abf7c-d281-4dbf-916b-267968104759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba63a4df-a804-4d99-98e7-187bd06f4a3e",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7cea7b-48f0-4178-a347-b285f46fb70a",
   "metadata": {},
   "source": [
    "### **Challenge 1: Build a Knowledge Base for AWS Certification**\n",
    "**Objective:** Create a knowledge base specifically for AWS Certification materials.\n",
    "\n",
    "**Scenario:**\n",
    "You are tasked with building a knowledge base to assist candidates preparing for the AWS Certification. The KB should store relevant materials, including certification guides, example questions, and printed web pages containing related content.\n",
    "\n",
    "Collect at least three PDFs for one of the certifications, such as:\n",
    "- AWS Cloud Practitioner\n",
    "- AWS Machine Learning Associate\n",
    "- AWS Machine Learning Specialty\n",
    "\n",
    "**Tasks:**\n",
    "1. **Data Preparation:**\n",
    "   - Download at least three PDFs relevant to the AWS Certification, such as:\n",
    "     - Certification guides.\n",
    "     - Practice exam questions.\n",
    "     - Study tips or FAQs from reputable sources.\n",
    "   - Save these files locally in a folder named `certifications_materials`.\n",
    "\n",
    "2. **Create OpenSearch Vector Index:**\n",
    "   - Write a Python script to:\n",
    "     - Set up an OpenSearch client using the **boto3** library.\n",
    "     - Create a vector index with at least one vector field for embeddings and one field for storing metadata (e.g., title or section).\n",
    "   - Use a unique name for the index, such as `certifications-index`.\n",
    "\n",
    "3. **Upload PDFs to S3 Bucket:**\n",
    "   - Write a Python script to:\n",
    "     - Create an S3 bucket programmatically.\n",
    "     - Upload the PDFs into the bucket under the `certifications-documents/` prefix.\n",
    "\n",
    "4. **Create the Knowledge Base in Amazon Bedrock:**\n",
    "   - Write Python code to:\n",
    "     - Set up the KB using Amazon Bedrock and link it to the OpenSearch collection.\n",
    "     - Ensure that embeddings are stored in the vector index created earlier.\n",
    "\n",
    "5. **Perform a Query Using the Retrieve API:**\n",
    "   - Write a Python script to query the KB for the following questions:\n",
    "     - *\"What are the key domains covered in the AWS Cloud Practitioner certification?\"*\n",
    "     - *\"What are the prerequisites for taking the AWS Machine Learning Specialty certification?\"*\n",
    "     - *\"Can you provide an example of a practice question for the AWS Machine Learning Associate certification?\"*\n",
    "\n",
    "**Deliverable:**\n",
    "- Python code that performs all tasks: creating the KB, indexing the data, and performing a retrieval query.\n",
    "- JSON file with the results of the two queries, including retrieved chunks and relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326c8d5-89a5-46ba-af8e-29d2e05492ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b5aecf-0fc1-4bd5-aa73-4fbffa10e240",
   "metadata": {},
   "source": [
    "### **Challenge 2: Build a Q&A System for AWS Solution Architect Certification**\n",
    "**Objective:** Create a Q&A system that generates answers based on the knowledge base created in Challenge 1.\n",
    "\n",
    "**Scenario:**\n",
    "You are building an intelligent assistant for AWS Certification candidates. The assistant should generate precise, contextually accurate answers using the RetrieveAndGenerate API.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Use the RetrieveAndGenerate API:**\n",
    "   - Write Python code to:\n",
    "     - Query the KB for context related to the user's question.\n",
    "     - Generate a response using the **Claude 3** model (or any other Bedrock model).\n",
    "     - Include the retrieved context in the model's prompt.\n",
    "\n",
    "2. **Create a Custom Prompt:**\n",
    "   - Write a custom prompt for the model to:\n",
    "     - Provide concise answers that incorporate numerical or statistical data from the retrieved context.\n",
    "     - Clearly cite the retrieved chunks as evidence in the response.\n",
    "   - Example prompt format:\n",
    "     ```\n",
    "     You are an AI assistant specializing in AWS Solution Architect Certification. Use the following context to answer the question:\n",
    "     Context:\n",
    "     {retrieved_context}\n",
    "     \n",
    "     Question:\n",
    "     {user_query}\n",
    "     \n",
    "     If the context does not provide enough information, respond with \"I don't have enough information to answer this question.\"\n",
    "     ```\n",
    "\n",
    "3. **Answer Sample Questions:**\n",
    "   - Generate answers for the following questions:\n",
    "     - *\"What are the key domains covered in the AWS Cloud Practitioner certification?\"*\n",
    "     - *\"What are the prerequisites for taking the AWS Machine Learning Specialty certification?\"*\n",
    "     - *\"Can you provide an example of a practice question for the AWS Machine Learning Associate certification?\"*\n",
    "\n",
    "4. **Handle Errors:**\n",
    "   - Implement error handling in Python for:\n",
    "     - Missing or insufficient retrieved context.\n",
    "     - API invocation failures.\n",
    "\n",
    "**Deliverable:**\n",
    "- Python script to query the KB and generate responses.\n",
    "- JSON file with the generated answers and retrieved evidence for the two sample questions.\n",
    "\n",
    "**Bonus:**\n",
    "- Use the `LangChain` to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d3c66-de2a-4ef4-8b9a-f8cd1a6dfc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cf9506d",
   "metadata": {},
   "source": [
    "### Conclusion and Next Steps\n",
    "\n",
    "The **Retrieve API** offers a powerful mechanism for customizing your **Retrieval-Augmented Generation (RAG)** applications. You can utilize the `InvokeModel` API from Amazon Bedrock or integrate it with LangChain using the `AmazonKnowledgeBaseRetriever`. This API enables flexibility in choosing the right foundation model from Amazon Bedrock and selecting the most suitable search type, whether **HYBRID** or **SEMANTIC**, tailored to your specific use case.\n",
    "\n",
    "For more information on the Hybrid Search feature, refer to the [official blog post](https://aws.amazon.com/blogs/machine-learning/knowledge-bases-for-amazon-bedrock-now-supports-hybrid-search/).\n",
    "\n",
    "#### Note on Resources\n",
    "\n",
    "It is essential to keep the CloudFormation stack and its associated resources intact for the next lab. The stack includes critical components such as the S3 bucket, OpenSearch collection, and other related resources required for subsequent exercises. \n",
    "\n",
    "If you are concerned about ongoing costs, ensure the resources are used exclusively for the labs and are not unnecessarily accessed outside of this environment.\n",
    "\n",
    "#### Reminder for Future Clean-Up\n",
    "\n",
    "Once all labs are completed, clean up provisioned resources to avoid incurring additional costs. At that time, you can delete the CloudFormation stack using the following code:\n",
    "\n",
    "```python\n",
    "cloudformation_client.delete_stack(StackName=stack_name)\n",
    "print(\"Stack deletion initiated.\")\n",
    "```\n",
    "\n",
    "Ensure you only perform this step after confirming that the resources are no longer required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e143292",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store stack_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe7b13-d171-4584-8fb9-07e18f7f812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store unique_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
